{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**bold text**\n",
        "\n",
        "\n",
        "\n",
        "# NLP ASSIGNMENT\n",
        "Shamiso Makainganwa\n",
        "R204437W"
      ],
      "metadata": {
        "id": "1Sy6JzJpNSfZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dUTuSHHUDMN5"
      },
      "outputs": [],
      "source": [
        "#importing and loading  all the libraries used in this assignment\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# LOADING THE SHONA TEXT FILE"
      ],
      "metadata": {
        "id": "-sXc2PyWNQsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\" Mazuva akafamba mai kim vachiita mabasa avo erima. Vakanga vave nemusika wavainge vaisa pagedhe repamba pavaigara. Waive musika waitengwa kwazvo, kunyanya fodya dzemudzanga. Musika waisanyanya kubhadhara asi ivo vaingoitira kuti murume wavo asaone mafambiro avaiita. Vakatenga mimwe midziyo mitsva nekuda kwemari dzechipfambi, ndokutengerawo murume mbatya dzakanaka dzekuti aende kunotsvagisa basa.\n",
        "Nekufamba kwemazuva, chipfambi chakanga zvino chachigara muropa ramai kim. Vaive vongoti chero wauya nemari wotopinda navo mumba zvisinei kuti mari yako imarii. Ndivo vakatanga nenyaya yekunzi dhora for 2 (dollar for two) kuEpworth. Vaive voita zvinhu zvinosemesa nekushoresa. Vana mai Gody vaimbenge vaedza kuvatsiura kuti vadzikame uye vasangodyorera nemarara asi ivo mai kim vaitoona sevaiitirwa jerasi. Nekudaro ushamwari hwavo hwakabva hwadzikira nekuti mai tee namai goddy vaiita tsoro dzakarongeka kwete zvamai kim zvekukumba nemasaga akabooka.\n",
        "Mai kim vakati rimwe zuva vari mundangariro \"asi sei ini nguva yese iyi ndanga ndichitambisa nguva. Chokwadi mari inonaka hayo. Mari isingarwadzi. Haina kana kumira mumutsetse wekubhengi. Kungovigirwa ndiri pano. Kim atosimba uyu. Baba vacho votochena. Iniwo ndadzokera kuve mai kim chaico. Kwete kuve scrap kwandanga ndaita kuye. Ndaisaziva kuti kune varume kunze uku kwete zvababa kim. Heeeede huri, ndosaka uchiti ukaona1dv mwana akatanga kuba dovi haazoregi. Chokwadi zve. Kunze uku kune vanoziva kiss. Kune vanoziva kuvaraidza vakadzi kwete zvanababa kim zvekungonhuhwa hapwa nemukanwa. Handifi ndakasiya basa iri. Ndiyo indigenisation plus women empowerment yacho inotaurwa naVaMugabe iyi. Pamberi nekuzvishandira. Uchizvidyira. Ohh nhasi wega kubva kuseni ndatove ne$61, ko kuzoti parichavirira. But problem, ko baba kim vakazvibata. But haaana kana basa. Murume wepi asingagoni kanakunditengera nzungu dzakakangwa? No. \"\n",
        "Rimwe zuva vakafunga kumbofonera shamwari yavo iya yavakambogara nayo mupark pamazuva ekurwara kwasisi getty. Foni yakadavirwa murume uya ndokutoti aizenge achisvikapo nenguva pfupi- pfupi. Hongu pasina nguva ainge atosvika. Akasvikopinda mumba mamai kim sezvo ukama nana mai Gody hwainge hwapera. Akaona mumba umu musina mubhedha ndokuudza mai Kim kuti aizozvigadzirisa musi uyu uye ndiye aida kurara pamubhedha wake kekutanga. Zvakatorera mai kim kuzvigamuchira nekuti baba kim vakanga vachizodzoka manheru emisi uyu.\n",
        "Vakaronga tsoro dzavo ndokuona kuti dzaizobudirira chete. Vapedza mutambo wavo vakabva vabuda vese kupinda mudhorobha reHarare umo mai Kim vakatengerwa kese kaidiwa mumba. Kubva kumasofa, hembe mubhedha, terevhozhoni nematorazuva maviri makuru(solar panels). Hepano pakubhadhara ndokubva card raramba kuita hameno hanzi network yaive down. Naizvozvo vakavinbisa kuti vaizodzoka pazuva raizotevera racho.\n",
        "Zvakasara mai Gody namai Tee kumba, vaive nekugunun'una kukuru pamusoro pehunhu hwamai kim.\n",
        "\"Asi kim atopanduka takatarisa mai Gody, hamuzvioniwo here?\"\n",
        "â€œChaida kuita kunge chisingahuri apa chiricho chipfambi chemhando. Ndidzo type dzakaroorwa dzapedza nyika idzi. Kunyepera kuziva MwRi zvenhando. \" mai Gody vakapindura.\n",
        "\" But azorasika zvakanyanya. Dai ari mumwe, dai asiyana nedzungu rake iri. Mhedzisiro kubaiwa kana kubaisa mumwe munhu.\" ndimai Tee avo. Mai. Gody ndokupindura vachiti \"Dai mwari apindira chokkwadi mwana panduka kuita ngorozi yaDhiyabhurosi chaiyo.. Uku baba kim vangu tiri murima hedu.\"\n",
        "\"Ende musi waachazviziva murume uya anofa hake. Ende ndipo panobva patoperera imba yacho\"\n",
        "\"Ndobva ndatomutora ini hangu . Ko ndingazotyei, ini ndiri single kudai. Vakadzi vari mudzimba vanototambisa varume kunge kutamba chikweshe vamwe tichivatsvaga kudai.. Unoti inyore here kurara wakatsamira gokora rako vamwe vachirara vakatsamirana? Munhu shingirira imba. Upenyu hwebhachura hunorema uye hunorwadza asikana\"\n",
        "\"Asikana musazodaro. Mati baba kim mungavagone here vaya. Ndivo anonzi maprayer warriors aye. Munhu akashinga pakunamata. Munhu asungadzoki kumashure\"\n",
        "Vakazodamburirwa nyaya dzavo nasis Nora avo vakavamhoresa,.\n",
        "Sis Nora musi uyu vakakasira kupedza basa ravo ndokuuya kumba kuzotsvairawo mumba mavo pamwe nekuwacha mbatya dzavo. Baba kim vazosvikawo rovira. Amai kim vaisavepo pamba. Vakatozopinda nguva dzatoenda. Kim ainge zvino anogoti gwavatata mumba nababa vake avo vaive vari ziii kunge nzuma iri kufunga chakadya nyanga. Vakaona kuti mai gody namai tee vainge vari pamba pavo adi wavo ega ndiye aisavepo. Vakazvibvunza mibvunzo yakawanda kudarika mhinduro asi akapindura ndiye akati wakandiisa riini? \"Kuti vaenda kunohodha? Vachisiya mumba musina kana kuwaridzwa magumbeze? Kusiya mwana akasuruvara kunge nyana rafirwa namai kudai. Iyo tsvina ine chana chacho.\" vakabva vatora mwana wavo ndokuisa muruoko ndokuenda kunochera mvura. Vakauya ndokugezesa mwana wavo. Dzakachaya 8 dzemanheru mai kim vasipo. Nhare yavo yaisadairwa. Hope kuna baba kim dzakashaikwa musi uyu vachifunga kwaive kwaenda mumwe wavo. Mai Kim vakatozosvika kunze kwave kuda kuchena kuma 2am.\n",
        "Baba kim vakabvunza mai kim kwavaive vaswera asi mhinduro dzacho dzaive dzekuti siyana neni.\n",
        "\"Baba kim siyana neni? Usandinetse. Kana uchiona hunhu hwangu hwachinja sekutaura kwako, the door is open move out and leave. And above all you are a manless. You cant even provide food for your only daughter, Kimberly. You cant afford to dress your wife and yourself. You are not a man to me anymore..From today onwards, iwe uchaita yako ini ndoita yangu. Ini ndakabva kumba kwedu ndisina mwana, kim will be in your hands, ini handina rudzi naye.\" Mai kim vaive vambomwiswa doro saka kurutsa mashoko kwaive mutserendende parurimi rwavo.\n",
        "Kim kaingochema zvako kakabatirira pana baba vako. Nemakore ako matatu iwayo kaitoita kunge kainzwa kunetsana kwababa nami vako. Kunetsana kwamai kim nemurume wavo mambakwedza iwayo kwazopedzisira vaviri ava varwa. Mai kim semunhu akange akuzikamwa akafumiroenda kukamba yemapurisa kunomhan'ara. Hazvina kutora nguva kuti baba kim vasungwe. Baba kim vakaswera vakavharirwa. Mamwe mapurisa ndiwo aivatuka. Vamwe vaivapawo mashoko anovaka. Mumwe mupurisa akanga achidanana namai kim uye ari mukuru wepakamba akabva apotsera ababa kim muchitokisi kuti agowana nguva yekufara yakanyatsokwana namai kim. Dzisiri nyasha dzaMwari dzakapindira, dai baba vaKim vakarara vachiitwa ropa ravo muto wetsikidzi. Baba kim vakaburitswa muusungwa naNora nemukomana wake Elvis. Nora aive anzwa kunetsana kwamai kim nababa kim, kubva pakusvika kwamai kim mambakwedza, kutukana, kusvika pakurwa. Akazofonera Elvis achimuudza zvese zvaive zvaitika. Elvis akabva angosvika nekutaura nemapurisa aivepo pacharge office pacho, ndokutobva baba kim vaburitswa. Hameno zvicard zvaakaratidza mapurisa acho. \"\"\""
      ],
      "metadata": {
        "id": "LTEs1DGvIOc8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "bJpaAyV1ccDP",
        "outputId": "151897e8-8f7f-477a-bf3a-d2620aed0f0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Mazuva akafamba mai kim vachiita mabasa avo erima. Vakanga vave nemusika wavainge vaisa pagedhe repamba pavaigara. Waive musika waitengwa kwazvo, kunyanya fodya dzemudzanga. Musika waisanyanya kubhadhara asi ivo vaingoitira kuti murume wavo asaone mafambiro avaiita. Vakatenga mimwe midziyo mitsva nekuda kwemari dzechipfambi, ndokutengerawo murume mbatya dzakanaka dzekuti aende kunotsvagisa basa. \\nNekufamba kwemazuva, chipfambi chakanga zvino chachigara muropa ramai kim. Vaive vongoti chero wauya nemari wotopinda navo mumba zvisinei kuti mari yako imarii. Ndivo vakatanga nenyaya yekunzi dhora for 2 (dollar for two) kuEpworth. Vaive voita zvinhu zvinosemesa nekushoresa. Vana mai Gody vaimbenge vaedza kuvatsiura kuti vadzikame uye vasangodyorera nemarara asi ivo mai kim vaitoona sevaiitirwa jerasi. Nekudaro ushamwari hwavo hwakabva hwadzikira nekuti mai tee namai goddy vaiita tsoro dzakarongeka kwete zvamai kim zvekukumba nemasaga akabooka. \\nMai kim vakati rimwe zuva vari mundangariro \"asi sei ini nguva yese iyi ndanga ndichitambisa nguva. Chokwadi mari inonaka hayo. Mari isingarwadzi. Haina kana kumira mumutsetse wekubhengi. Kungovigirwa ndiri pano. Kim atosimba uyu. Baba vacho votochena. Iniwo ndadzokera kuve mai kim chaico. Kwete kuve scrap kwandanga ndaita kuye. Ndaisaziva kuti kune varume kunze uku kwete zvababa kim. Heeeede huri, ndosaka uchiti ukaona1dv mwana akatanga kuba dovi haazoregi. Chokwadi zve. Kunze uku kune vanoziva kiss. Kune vanoziva kuvaraidza vakadzi kwete zvanababa kim zvekungonhuhwa hapwa nemukanwa. Handifi ndakasiya basa iri. Ndiyo indigenisation plus women empowerment yacho inotaurwa naVaMugabe iyi. Pamberi nekuzvishandira. Uchizvidyira. Ohh nhasi wega kubva kuseni ndatove ne$61, ko kuzoti parichavirira. But problem, ko baba kim vakazvibata. But haaana kana basa. Murume wepi asingagoni kanakunditengera nzungu dzakakangwa? No. \"\\nRimwe zuva vakafunga kumbofonera shamwari yavo iya yavakambogara nayo mupark pamazuva ekurwara kwasisi getty. Foni yakadavirwa murume uya ndokutoti aizenge achisvikapo nenguva pfupi- pfupi. Hongu pasina nguva ainge atosvika. Akasvikopinda mumba mamai kim sezvo ukama nana mai Gody hwainge hwapera. Akaona mumba umu musina mubhedha ndokuudza mai Kim kuti aizozvigadzirisa musi uyu uye ndiye aida kurara pamubhedha wake kekutanga. Zvakatorera mai kim kuzvigamuchira nekuti baba kim vakanga vachizodzoka manheru emisi uyu.\\nVakaronga tsoro dzavo ndokuona kuti dzaizobudirira chete. Vapedza mutambo wavo vakabva vabuda vese kupinda mudhorobha reHarare umo mai Kim vakatengerwa kese kaidiwa mumba. Kubva kumasofa, hembe mubhedha, terevhozhoni nematorazuva maviri makuru(solar panels). Hepano pakubhadhara ndokubva card raramba kuita hameno hanzi network yaive down. Naizvozvo vakavinbisa kuti vaizodzoka pazuva raizotevera racho.\\nZvakasara mai Gody namai Tee kumba, vaive nekugunun\\'una kukuru pamusoro pehunhu hwamai kim.\\n\"Asi kim atopanduka takatarisa mai Gody, hamuzvioniwo here?\"\\nâ€œChaida kuita kunge chisingahuri apa chiricho chipfambi chemhando. Ndidzo type dzakaroorwa dzapedza nyika idzi. Kunyepera kuziva MwRi zvenhando. \" mai Gody vakapindura.\\n\" But azorasika zvakanyanya. Dai ari mumwe, dai asiyana nedzungu rake iri. Mhedzisiro kubaiwa kana kubaisa mumwe munhu.\" ndimai Tee avo. Mai. Gody ndokupindura vachiti \"Dai mwari apindira chokkwadi mwana panduka kuita ngorozi yaDhiyabhurosi chaiyo.. Uku baba kim vangu tiri murima hedu.\"\\n\"Ende musi waachazviziva murume uya anofa hake. Ende ndipo panobva patoperera imba yacho\"\\n\"Ndobva ndatomutora ini hangu . Ko ndingazotyei, ini ndiri single kudai. Vakadzi vari mudzimba vanototambisa varume kunge kutamba chikweshe vamwe tichivatsvaga kudai.. Unoti inyore here kurara wakatsamira gokora rako vamwe vachirara vakatsamirana? Munhu shingirira imba. Upenyu hwebhachura hunorema uye hunorwadza asikana\"\\n\"Asikana musazodaro. Mati baba kim mungavagone here vaya. Ndivo anonzi maprayer warriors aye. Munhu akashinga pakunamata. Munhu asungadzoki kumashure\"\\nVakazodamburirwa nyaya dzavo nasis Nora avo vakavamhoresa,.\\nSis Nora musi uyu vakakasira kupedza basa ravo ndokuuya kumba kuzotsvairawo mumba mavo pamwe nekuwacha mbatya dzavo. Baba kim vazosvikawo rovira. Amai kim vaisavepo pamba. Vakatozopinda nguva dzatoenda. Kim ainge zvino anogoti gwavatata mumba nababa vake avo vaive vari ziii kunge nzuma iri kufunga chakadya nyanga. Vakaona kuti mai gody namai tee vainge vari pamba pavo adi wavo ega ndiye aisavepo. Vakazvibvunza mibvunzo yakawanda kudarika mhinduro asi akapindura ndiye akati wakandiisa riini? \"Kuti vaenda kunohodha? Vachisiya mumba musina kana kuwaridzwa magumbeze? Kusiya mwana akasuruvara kunge nyana rafirwa namai kudai. Iyo tsvina ine chana chacho.\" vakabva vatora mwana wavo ndokuisa muruoko ndokuenda kunochera mvura. Vakauya ndokugezesa mwana wavo. Dzakachaya 8 dzemanheru mai kim vasipo. Nhare yavo yaisadairwa. Hope kuna baba kim dzakashaikwa musi uyu vachifunga kwaive kwaenda mumwe wavo. Mai Kim vakatozosvika kunze kwave kuda kuchena kuma 2am.\\nBaba kim vakabvunza mai kim kwavaive vaswera asi mhinduro dzacho dzaive dzekuti siyana neni.\\n\"Baba kim siyana neni? Usandinetse. Kana uchiona hunhu hwangu hwachinja sekutaura kwako, the door is open move out and leave. And above all you are a manless. You cant even provide food for your only daughter, Kimberly. You cant afford to dress your wife and yourself. You are not a man to me anymore..From today onwards, iwe uchaita yako ini ndoita yangu. Ini ndakabva kumba kwedu ndisina mwana, kim will be in your hands, ini handina rudzi naye.\" Mai kim vaive vambomwiswa doro saka kurutsa mashoko kwaive mutserendende parurimi rwavo.\\nKim kaingochema zvako kakabatirira pana baba vako. Nemakore ako matatu iwayo kaitoita kunge kainzwa kunetsana kwababa nami vako. Kunetsana kwamai kim nemurume wavo mambakwedza iwayo kwazopedzisira vaviri ava varwa. Mai kim semunhu akange akuzikamwa akafumiroenda kukamba yemapurisa kunomhan\\'ara. Hazvina kutora nguva kuti baba kim vasungwe. Baba kim vakaswera vakavharirwa. Mamwe mapurisa ndiwo aivatuka. Vamwe vaivapawo mashoko anovaka. Mumwe mupurisa akanga achidanana namai kim uye ari mukuru wepakamba akabva apotsera ababa kim muchitokisi kuti agowana nguva yekufara yakanyatsokwana namai kim. Dzisiri nyasha dzaMwari dzakapindira, dai baba vaKim vakarara vachiitwa ropa ravo muto wetsikidzi. Baba kim vakaburitswa muusungwa naNora nemukomana wake Elvis. Nora aive anzwa kunetsana kwamai kim nababa kim, kubva pakusvika kwamai kim mambakwedza, kutukana, kusvika pakurwa. Akazofonera Elvis achimuudza zvese zvaive zvaitika. Elvis akabva angosvika nekutaura nemapurisa aivepo pacharge office pacho, ndokutobva baba kim vaburitswa. Hameno zvicard zvaakaratidza mapurisa acho. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hkK0UA4fOOss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " PREPROCESSING TEXT"
      ],
      "metadata": {
        "id": "iXWgzEv2OPgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXpRgfR-NG5M",
        "outputId": "034c3aec-0d01-411a-dce8-903dba78b104"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Join the words back into a cleaned text\n",
        "    cleaned_text = ' '.join(words)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "cleaned_text = clean_text(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIOZjSCO6Fh0",
        "outputId": "875decf2-ff89-4669-f17a-e9376d4d0d52"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "WQK9Pr3460CD",
        "outputId": "fda7f1a5-9b98-445a-fec7-27df17df858e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mazuva akafamba mai kim vachiita mabasa avo erima vakanga vave nemusika wavainge vaisa pagedhe repamba pavaigara waive musika waitengwa kwazvo kunyanya fodya dzemudzanga musika waisanyanya kubhadhara asi ivo vaingoitira kuti murume wavo asaone mafambiro avaiita vakatenga mimwe midziyo mitsva nekuda kwemari dzechipfambi ndokutengerawo murume mbatya dzakanaka dzekuti aende kunotsvagisa basa nekufamba kwemazuva chipfambi chakanga zvino chachigara muropa ramai kim vaive vongoti chero wauya nemari wotopinda navo mumba zvisinei kuti mari yako imarii ndivo vakatanga nenyaya yekunzi dhora 2 dollar two kuepworth vaive voita zvinhu zvinosemesa nekushoresa vana mai gody vaimbenge vaedza kuvatsiura kuti vadzikame uye vasangodyorera nemarara asi ivo mai kim vaitoona sevaiitirwa jerasi nekudaro ushamwari hwavo hwakabva hwadzikira nekuti mai tee namai goddy vaiita tsoro dzakarongeka kwete zvamai kim zvekukumba nemasaga akabooka mai kim vakati rimwe zuva vari mundangariro asi sei ini nguva yese iyi ndanga ndichitambisa nguva chokwadi mari inonaka hayo mari isingarwadzi haina kana kumira mumutsetse wekubhengi kungovigirwa ndiri pano kim atosimba uyu baba vacho votochena iniwo ndadzokera kuve mai kim chaico kwete kuve scrap kwandanga ndaita kuye ndaisaziva kuti kune varume kunze uku kwete zvababa kim heeeede huri ndosaka uchiti ukaona1dv mwana akatanga kuba dovi haazoregi chokwadi zve kunze uku kune vanoziva kiss kune vanoziva kuvaraidza vakadzi kwete zvanababa kim zvekungonhuhwa hapwa nemukanwa handifi ndakasiya basa iri ndiyo indigenisation plus women empowerment yacho inotaurwa navamugabe iyi pamberi nekuzvishandira uchizvidyira ohh nhasi wega kubva kuseni ndatove ne61 ko kuzoti parichavirira problem ko baba kim vakazvibata haaana kana basa murume wepi asingagoni kanakunditengera nzungu dzakakangwa rimwe zuva vakafunga kumbofonera shamwari yavo iya yavakambogara nayo mupark pamazuva ekurwara kwasisi getty foni yakadavirwa murume uya ndokutoti aizenge achisvikapo nenguva pfupi pfupi hongu pasina nguva ainge atosvika akasvikopinda mumba mamai kim sezvo ukama nana mai gody hwainge hwapera akaona mumba umu musina mubhedha ndokuudza mai kim kuti aizozvigadzirisa musi uyu uye ndiye aida kurara pamubhedha wake kekutanga zvakatorera mai kim kuzvigamuchira nekuti baba kim vakanga vachizodzoka manheru emisi uyu vakaronga tsoro dzavo ndokuona kuti dzaizobudirira chete vapedza mutambo wavo vakabva vabuda vese kupinda mudhorobha reharare umo mai kim vakatengerwa kese kaidiwa mumba kubva kumasofa hembe mubhedha terevhozhoni nematorazuva maviri makurusolar panels hepano pakubhadhara ndokubva card raramba kuita hameno hanzi network yaive naizvozvo vakavinbisa kuti vaizodzoka pazuva raizotevera racho zvakasara mai gody namai tee kumba vaive nekugunununa kukuru pamusoro pehunhu hwamai kim asi kim atopanduka takatarisa mai gody hamuzvioniwo chaida kuita kunge chisingahuri apa chiricho chipfambi chemhando ndidzo type dzakaroorwa dzapedza nyika idzi kunyepera kuziva mwri zvenhando mai gody vakapindura azorasika zvakanyanya dai ari mumwe dai asiyana nedzungu rake iri mhedzisiro kubaiwa kana kubaisa mumwe munhu ndimai tee avo mai gody ndokupindura vachiti dai mwari apindira chokkwadi mwana panduka kuita ngorozi yadhiyabhurosi chaiyo uku baba kim vangu tiri murima hedu ende musi waachazviziva murume uya anofa hake ende ndipo panobva patoperera imba yacho ndobva ndatomutora ini hangu ko ndingazotyei ini ndiri single kudai vakadzi vari mudzimba vanototambisa varume kunge kutamba chikweshe vamwe tichivatsvaga kudai unoti inyore kurara wakatsamira gokora rako vamwe vachirara vakatsamirana munhu shingirira imba upenyu hwebhachura hunorema uye hunorwadza asikana asikana musazodaro mati baba kim mungavagone vaya ndivo anonzi maprayer warriors aye munhu akashinga pakunamata munhu asungadzoki kumashure vakazodamburirwa nyaya dzavo nasis nora avo vakavamhoresa sis nora musi uyu vakakasira kupedza basa ravo ndokuuya kumba kuzotsvairawo mumba mavo pamwe nekuwacha mbatya dzavo baba kim vazosvikawo rovira amai kim vaisavepo pamba vakatozopinda nguva dzatoenda kim ainge zvino anogoti gwavatata mumba nababa vake avo vaive vari ziii kunge nzuma iri kufunga chakadya nyanga vakaona kuti mai gody namai tee vainge vari pamba pavo adi wavo ega ndiye aisavepo vakazvibvunza mibvunzo yakawanda kudarika mhinduro asi akapindura ndiye akati wakandiisa riini kuti vaenda kunohodha vachisiya mumba musina kana kuwaridzwa magumbeze kusiya mwana akasuruvara kunge nyana rafirwa namai kudai iyo tsvina ine chana chacho vakabva vatora mwana wavo ndokuisa muruoko ndokuenda kunochera mvura vakauya ndokugezesa mwana wavo dzakachaya 8 dzemanheru mai kim vasipo nhare yavo yaisadairwa hope kuna baba kim dzakashaikwa musi uyu vachifunga kwaive kwaenda mumwe wavo mai kim vakatozosvika kunze kwave kuda kuchena kuma 2am baba kim vakabvunza mai kim kwavaive vaswera asi mhinduro dzacho dzaive dzekuti siyana neni baba kim siyana neni usandinetse kana uchiona hunhu hwangu hwachinja sekutaura kwako door open move leave manless cant even provide food daughter kimberly cant afford dress wife man anymorefrom today onwards iwe uchaita yako ini ndoita yangu ini ndakabva kumba kwedu ndisina mwana kim hands ini handina rudzi naye mai kim vaive vambomwiswa doro saka kurutsa mashoko kwaive mutserendende parurimi rwavo kim kaingochema zvako kakabatirira pana baba vako nemakore ako matatu iwayo kaitoita kunge kainzwa kunetsana kwababa nami vako kunetsana kwamai kim nemurume wavo mambakwedza iwayo kwazopedzisira vaviri ava varwa mai kim semunhu akange akuzikamwa akafumiroenda kukamba yemapurisa kunomhanara hazvina kutora nguva kuti baba kim vasungwe baba kim vakaswera vakavharirwa mamwe mapurisa ndiwo aivatuka vamwe vaivapawo mashoko anovaka mumwe mupurisa akanga achidanana namai kim uye ari mukuru wepakamba akabva apotsera ababa kim muchitokisi kuti agowana nguva yekufara yakanyatsokwana namai kim dzisiri nyasha dzamwari dzakapindira dai baba vakim vakarara vachiitwa ropa ravo muto wetsikidzi baba kim vakaburitswa muusungwa nanora nemukomana wake elvis nora aive anzwa kunetsana kwamai kim nababa kim kubva pakusvika kwamai kim mambakwedza kutukana kusvika pakurwa akazofonera elvis achimuudza zvese zvaive zvaitika elvis akabva angosvika nekutaura nemapurisa aivepo pacharge office pacho ndokutobva baba kim vaburitswa hameno zvicard zvaakaratidza mapurisa acho'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dfdffef8"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Tokenize and preprocess\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([cleaned_text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# tokenize the document\n",
        "result = text_to_word_sequence(text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2bXEdKZl5wM",
        "outputId": "99a3d259-4d7a-4f84-ca43-216363b95782"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mazuva', 'akafamba', 'mai', 'kim', 'vachiita', 'mabasa', 'avo', 'erima', 'vakanga', 'vave', 'nemusika', 'wavainge', 'vaisa', 'pagedhe', 'repamba', 'pavaigara', 'waive', 'musika', 'waitengwa', 'kwazvo', 'kunyanya', 'fodya', 'dzemudzanga', 'musika', 'waisanyanya', 'kubhadhara', 'asi', 'ivo', 'vaingoitira', 'kuti', 'murume', 'wavo', 'asaone', 'mafambiro', 'avaiita', 'vakatenga', 'mimwe', 'midziyo', 'mitsva', 'nekuda', 'kwemari', 'dzechipfambi', 'ndokutengerawo', 'murume', 'mbatya', 'dzakanaka', 'dzekuti', 'aende', 'kunotsvagisa', 'basa', 'nekufamba', 'kwemazuva', 'chipfambi', 'chakanga', 'zvino', 'chachigara', 'muropa', 'ramai', 'kim', 'vaive', 'vongoti', 'chero', 'wauya', 'nemari', 'wotopinda', 'navo', 'mumba', 'zvisinei', 'kuti', 'mari', 'yako', 'imarii', 'ndivo', 'vakatanga', 'nenyaya', 'yekunzi', 'dhora', 'for', '2', 'dollar', 'for', 'two', 'kuepworth', 'vaive', 'voita', 'zvinhu', 'zvinosemesa', 'nekushoresa', 'vana', 'mai', 'gody', 'vaimbenge', 'vaedza', 'kuvatsiura', 'kuti', 'vadzikame', 'uye', 'vasangodyorera', 'nemarara', 'asi', 'ivo', 'mai', 'kim', 'vaitoona', 'sevaiitirwa', 'jerasi', 'nekudaro', 'ushamwari', 'hwavo', 'hwakabva', 'hwadzikira', 'nekuti', 'mai', 'tee', 'namai', 'goddy', 'vaiita', 'tsoro', 'dzakarongeka', 'kwete', 'zvamai', 'kim', 'zvekukumba', 'nemasaga', 'akabooka', 'mai', 'kim', 'vakati', 'rimwe', 'zuva', 'vari', 'mundangariro', 'asi', 'sei', 'ini', 'nguva', 'yese', 'iyi', 'ndanga', 'ndichitambisa', 'nguva', 'chokwadi', 'mari', 'inonaka', 'hayo', 'mari', 'isingarwadzi', 'haina', 'kana', 'kumira', 'mumutsetse', 'wekubhengi', 'kungovigirwa', 'ndiri', 'pano', 'kim', 'atosimba', 'uyu', 'baba', 'vacho', 'votochena', 'iniwo', 'ndadzokera', 'kuve', 'mai', 'kim', 'chaico', 'kwete', 'kuve', 'scrap', 'kwandanga', 'ndaita', 'kuye', 'ndaisaziva', 'kuti', 'kune', 'varume', 'kunze', 'uku', 'kwete', 'zvababa', 'kim', 'heeeede', 'huri', 'ndosaka', 'uchiti', 'ukaona1dv', 'mwana', 'akatanga', 'kuba', 'dovi', 'haazoregi', 'chokwadi', 'zve', 'kunze', 'uku', 'kune', 'vanoziva', 'kiss', 'kune', 'vanoziva', 'kuvaraidza', 'vakadzi', 'kwete', 'zvanababa', 'kim', 'zvekungonhuhwa', 'hapwa', 'nemukanwa', 'handifi', 'ndakasiya', 'basa', 'iri', 'ndiyo', 'indigenisation', 'plus', 'women', 'empowerment', 'yacho', 'inotaurwa', 'navamugabe', 'iyi', 'pamberi', 'nekuzvishandira', 'uchizvidyira', 'ohh', 'nhasi', 'wega', 'kubva', 'kuseni', 'ndatove', 'ne', '61', 'ko', 'kuzoti', 'parichavirira', 'but', 'problem', 'ko', 'baba', 'kim', 'vakazvibata', 'but', 'haaana', 'kana', 'basa', 'murume', 'wepi', 'asingagoni', 'kanakunditengera', 'nzungu', 'dzakakangwa', 'no', 'rimwe', 'zuva', 'vakafunga', 'kumbofonera', 'shamwari', 'yavo', 'iya', 'yavakambogara', 'nayo', 'mupark', 'pamazuva', 'ekurwara', 'kwasisi', 'getty', 'foni', 'yakadavirwa', 'murume', 'uya', 'ndokutoti', 'aizenge', 'achisvikapo', 'nenguva', 'pfupi', 'pfupi', 'hongu', 'pasina', 'nguva', 'ainge', 'atosvika', 'akasvikopinda', 'mumba', 'mamai', 'kim', 'sezvo', 'ukama', 'nana', 'mai', 'gody', 'hwainge', 'hwapera', 'akaona', 'mumba', 'umu', 'musina', 'mubhedha', 'ndokuudza', 'mai', 'kim', 'kuti', 'aizozvigadzirisa', 'musi', 'uyu', 'uye', 'ndiye', 'aida', 'kurara', 'pamubhedha', 'wake', 'kekutanga', 'zvakatorera', 'mai', 'kim', 'kuzvigamuchira', 'nekuti', 'baba', 'kim', 'vakanga', 'vachizodzoka', 'manheru', 'emisi', 'uyu', 'vakaronga', 'tsoro', 'dzavo', 'ndokuona', 'kuti', 'dzaizobudirira', 'chete', 'vapedza', 'mutambo', 'wavo', 'vakabva', 'vabuda', 'vese', 'kupinda', 'mudhorobha', 'reharare', 'umo', 'mai', 'kim', 'vakatengerwa', 'kese', 'kaidiwa', 'mumba', 'kubva', 'kumasofa', 'hembe', 'mubhedha', 'terevhozhoni', 'nematorazuva', 'maviri', 'makuru', 'solar', 'panels', 'hepano', 'pakubhadhara', 'ndokubva', 'card', 'raramba', 'kuita', 'hameno', 'hanzi', 'network', 'yaive', 'down', 'naizvozvo', 'vakavinbisa', 'kuti', 'vaizodzoka', 'pazuva', 'raizotevera', 'racho', 'zvakasara', 'mai', 'gody', 'namai', 'tee', 'kumba', 'vaive', \"nekugunun'una\", 'kukuru', 'pamusoro', 'pehunhu', 'hwamai', 'kim', 'asi', 'kim', 'atopanduka', 'takatarisa', 'mai', 'gody', 'hamuzvioniwo', 'here', 'â€œchaida', 'kuita', 'kunge', 'chisingahuri', 'apa', 'chiricho', 'chipfambi', 'chemhando', 'ndidzo', 'type', 'dzakaroorwa', 'dzapedza', 'nyika', 'idzi', 'kunyepera', 'kuziva', 'mwri', 'zvenhando', 'mai', 'gody', 'vakapindura', 'but', 'azorasika', 'zvakanyanya', 'dai', 'ari', 'mumwe', 'dai', 'asiyana', 'nedzungu', 'rake', 'iri', 'mhedzisiro', 'kubaiwa', 'kana', 'kubaisa', 'mumwe', 'munhu', 'ndimai', 'tee', 'avo', 'mai', 'gody', 'ndokupindura', 'vachiti', 'dai', 'mwari', 'apindira', 'chokkwadi', 'mwana', 'panduka', 'kuita', 'ngorozi', 'yadhiyabhurosi', 'chaiyo', 'uku', 'baba', 'kim', 'vangu', 'tiri', 'murima', 'hedu', 'ende', 'musi', 'waachazviziva', 'murume', 'uya', 'anofa', 'hake', 'ende', 'ndipo', 'panobva', 'patoperera', 'imba', 'yacho', 'ndobva', 'ndatomutora', 'ini', 'hangu', 'ko', 'ndingazotyei', 'ini', 'ndiri', 'single', 'kudai', 'vakadzi', 'vari', 'mudzimba', 'vanototambisa', 'varume', 'kunge', 'kutamba', 'chikweshe', 'vamwe', 'tichivatsvaga', 'kudai', 'unoti', 'inyore', 'here', 'kurara', 'wakatsamira', 'gokora', 'rako', 'vamwe', 'vachirara', 'vakatsamirana', 'munhu', 'shingirira', 'imba', 'upenyu', 'hwebhachura', 'hunorema', 'uye', 'hunorwadza', 'asikana', 'asikana', 'musazodaro', 'mati', 'baba', 'kim', 'mungavagone', 'here', 'vaya', 'ndivo', 'anonzi', 'maprayer', 'warriors', 'aye', 'munhu', 'akashinga', 'pakunamata', 'munhu', 'asungadzoki', 'kumashure', 'vakazodamburirwa', 'nyaya', 'dzavo', 'nasis', 'nora', 'avo', 'vakavamhoresa', 'sis', 'nora', 'musi', 'uyu', 'vakakasira', 'kupedza', 'basa', 'ravo', 'ndokuuya', 'kumba', 'kuzotsvairawo', 'mumba', 'mavo', 'pamwe', 'nekuwacha', 'mbatya', 'dzavo', 'baba', 'kim', 'vazosvikawo', 'rovira', 'amai', 'kim', 'vaisavepo', 'pamba', 'vakatozopinda', 'nguva', 'dzatoenda', 'kim', 'ainge', 'zvino', 'anogoti', 'gwavatata', 'mumba', 'nababa', 'vake', 'avo', 'vaive', 'vari', 'ziii', 'kunge', 'nzuma', 'iri', 'kufunga', 'chakadya', 'nyanga', 'vakaona', 'kuti', 'mai', 'gody', 'namai', 'tee', 'vainge', 'vari', 'pamba', 'pavo', 'adi', 'wavo', 'ega', 'ndiye', 'aisavepo', 'vakazvibvunza', 'mibvunzo', 'yakawanda', 'kudarika', 'mhinduro', 'asi', 'akapindura', 'ndiye', 'akati', 'wakandiisa', 'riini', 'kuti', 'vaenda', 'kunohodha', 'vachisiya', 'mumba', 'musina', 'kana', 'kuwaridzwa', 'magumbeze', 'kusiya', 'mwana', 'akasuruvara', 'kunge', 'nyana', 'rafirwa', 'namai', 'kudai', 'iyo', 'tsvina', 'ine', 'chana', 'chacho', 'vakabva', 'vatora', 'mwana', 'wavo', 'ndokuisa', 'muruoko', 'ndokuenda', 'kunochera', 'mvura', 'vakauya', 'ndokugezesa', 'mwana', 'wavo', 'dzakachaya', '8', 'dzemanheru', 'mai', 'kim', 'vasipo', 'nhare', 'yavo', 'yaisadairwa', 'hope', 'kuna', 'baba', 'kim', 'dzakashaikwa', 'musi', 'uyu', 'vachifunga', 'kwaive', 'kwaenda', 'mumwe', 'wavo', 'mai', 'kim', 'vakatozosvika', 'kunze', 'kwave', 'kuda', 'kuchena', 'kuma', '2am', 'baba', 'kim', 'vakabvunza', 'mai', 'kim', 'kwavaive', 'vaswera', 'asi', 'mhinduro', 'dzacho', 'dzaive', 'dzekuti', 'siyana', 'neni', 'baba', 'kim', 'siyana', 'neni', 'usandinetse', 'kana', 'uchiona', 'hunhu', 'hwangu', 'hwachinja', 'sekutaura', 'kwako', 'the', 'door', 'is', 'open', 'move', 'out', 'and', 'leave', 'and', 'above', 'all', 'you', 'are', 'a', 'manless', 'you', 'cant', 'even', 'provide', 'food', 'for', 'your', 'only', 'daughter', 'kimberly', 'you', 'cant', 'afford', 'to', 'dress', 'your', 'wife', 'and', 'yourself', 'you', 'are', 'not', 'a', 'man', 'to', 'me', 'anymore', 'from', 'today', 'onwards', 'iwe', 'uchaita', 'yako', 'ini', 'ndoita', 'yangu', 'ini', 'ndakabva', 'kumba', 'kwedu', 'ndisina', 'mwana', 'kim', 'will', 'be', 'in', 'your', 'hands', 'ini', 'handina', 'rudzi', 'naye', 'mai', 'kim', 'vaive', 'vambomwiswa', 'doro', 'saka', 'kurutsa', 'mashoko', 'kwaive', 'mutserendende', 'parurimi', 'rwavo', 'kim', 'kaingochema', 'zvako', 'kakabatirira', 'pana', 'baba', 'vako', 'nemakore', 'ako', 'matatu', 'iwayo', 'kaitoita', 'kunge', 'kainzwa', 'kunetsana', 'kwababa', 'nami', 'vako', 'kunetsana', 'kwamai', 'kim', 'nemurume', 'wavo', 'mambakwedza', 'iwayo', 'kwazopedzisira', 'vaviri', 'ava', 'varwa', 'mai', 'kim', 'semunhu', 'akange', 'akuzikamwa', 'akafumiroenda', 'kukamba', 'yemapurisa', \"kunomhan'ara\", 'hazvina', 'kutora', 'nguva', 'kuti', 'baba', 'kim', 'vasungwe', 'baba', 'kim', 'vakaswera', 'vakavharirwa', 'mamwe', 'mapurisa', 'ndiwo', 'aivatuka', 'vamwe', 'vaivapawo', 'mashoko', 'anovaka', 'mumwe', 'mupurisa', 'akanga', 'achidanana', 'namai', 'kim', 'uye', 'ari', 'mukuru', 'wepakamba', 'akabva', 'apotsera', 'ababa', 'kim', 'muchitokisi', 'kuti', 'agowana', 'nguva', 'yekufara', 'yakanyatsokwana', 'namai', 'kim', 'dzisiri', 'nyasha', 'dzamwari', 'dzakapindira', 'dai', 'baba', 'vakim', 'vakarara', 'vachiitwa', 'ropa', 'ravo', 'muto', 'wetsikidzi', 'baba', 'kim', 'vakaburitswa', 'muusungwa', 'nanora', 'nemukomana', 'wake', 'elvis', 'nora', 'aive', 'anzwa', 'kunetsana', 'kwamai', 'kim', 'nababa', 'kim', 'kubva', 'pakusvika', 'kwamai', 'kim', 'mambakwedza', 'kutukana', 'kusvika', 'pakurwa', 'akazofonera', 'elvis', 'achimuudza', 'zvese', 'zvaive', 'zvaitika', 'elvis', 'akabva', 'angosvika', 'nekutaura', 'nemapurisa', 'aivepo', 'pacharge', 'office', 'pacho', 'ndokutobva', 'baba', 'kim', 'vaburitswa', 'hameno', 'zvicard', 'zvaakaratidza', 'mapurisa', 'acho']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# saving\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "9jLfyavKWDG3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        " BUILDING THE VOCABULARY"
      ],
      "metadata": {
        "id": "eLfMQCsFOeqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using a keras tokenizer to build an optimal vocabulary\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text)\n",
        "max_vocab_size= 10000  # the vocabulary size\n",
        "word_index = tokenizer.word_index\n",
        "vocabulary_size = min(max_vocab_size, len(word_index))\n",
        "\n",
        "\n",
        "reduced_word_index = {}\n",
        "for word, index in word_index.items():\n",
        "    if index <= vocabulary_size:\n",
        "        reduced_word_index[word] = index\n",
        "    else:\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "tokenizer.word_index = reduced_word_index\n",
        "tokenizer.word_index[tokenizer.oov_token] = vocabulary_size + 1\n",
        "tokenizer.num_words = vocabulary_size + 1\n",
        "vocabulary_size = len(word_index)\n",
        "print(\"Vocabulary size:\", vocabulary_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUQjl1jtM0vp",
        "outputId": "48a8fe0f-6947-41cf-885f-09a173fdf613"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WORD EMBEDDINGS"
      ],
      "metadata": {
        "id": "dwAmgyzuOvTF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "f6e79b35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e51fde3f-ce39-4caf-981e-5b4e54af47cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train word2vec model with gensim\n",
        "sentences = result\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "model.save(\"shona_embeddings.model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BI-DIRECTIONAL MODEL WITH EMBEDDING LAYER"
      ],
      "metadata": {
        "id": "gPENFQKAO6lK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8095c8fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a7a0ccc-969c-4fe5-f22b-408b987130eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 5, 100)            61700     \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 5, 300)            301200    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5, 300)            0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 617)               62317     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 585617 (2.23 MB)\n",
            "Trainable params: 585617 (2.23 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(total_words, 100, input_length=5))\n",
        "model1.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(LSTM(100))\n",
        "model1.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.save('model1.h1')"
      ],
      "metadata": {
        "id": "xAYrk6_m-Bkr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BI-DIRECTIONAL MODEL WITH PRETRAINED WORD EMBEDDINGS"
      ],
      "metadata": {
        "id": "_XH2cOyPPFOi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "90591dd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7412c0c-f827-462e-c50d-9705bf8c2955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 5, 100)            61700     \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 5, 300)            301200    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 5, 300)            0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 617)               62317     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 585617 (2.23 MB)\n",
            "Trainable params: 523917 (2.00 MB)\n",
            "Non-trainable params: 61700 (241.02 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Load the word2vec model\n",
        "import numpy as np\n",
        "model = Word2Vec.load(\"shona_embeddings.model\")\n",
        "embedding_matrix = np.zeros((total_words, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    try:\n",
        "        embedding_vector = reduced_word_index[word]\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        # Word not present in gensim model, a zero embedding will be used for this word\n",
        "        pass\n",
        "\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(total_words, 100, weights=[embedding_matrix], input_length=5, trainable=False))\n",
        "model2.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(LSTM(100))\n",
        "model2.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.save('model2.h1')"
      ],
      "metadata": {
        "id": "W47ls8Yb-rTv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = load_model('model1.h1')\n",
        "model = load_model('model2.h1')\n",
        "\n"
      ],
      "metadata": {
        "id": "il0czMAH-8w4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL TRAINING"
      ],
      "metadata": {
        "id": "Q33I0hUzPTqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trained using hold out cross validation**"
      ],
      "metadata": {
        "id": "bQd86usEcov4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb129419",
        "outputId": "43ea80ab-1ca4-43c7-9232-e0b688279f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training model1\n",
            "Epoch 1/150\n",
            "5/5 [==============================] - 10s 594ms/step - loss: 5.3191 - accuracy: 0.0000e+00 - val_loss: 5.3220 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/150\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 5.3081 - accuracy: 0.1127 - val_loss: 5.3268 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/150\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 5.2973 - accuracy: 0.1056 - val_loss: 5.3344 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/150\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 5.2828 - accuracy: 0.1056 - val_loss: 5.3476 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/150\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 5.2589 - accuracy: 0.0915 - val_loss: 5.3780 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/150\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 5.2105 - accuracy: 0.0634 - val_loss: 5.4695 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 5.0939 - accuracy: 0.0423 - val_loss: 5.8941 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/150\n",
            "5/5 [==============================] - 0s 49ms/step - loss: 4.8959 - accuracy: 0.0211 - val_loss: 7.1113 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/150\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 4.7781 - accuracy: 0.0211 - val_loss: 7.4115 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 4.5986 - accuracy: 0.0423 - val_loss: 7.1527 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/150\n",
            "5/5 [==============================] - 0s 61ms/step - loss: 4.4049 - accuracy: 0.0493 - val_loss: 7.4059 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/150\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 4.2079 - accuracy: 0.0704 - val_loss: 7.9255 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/150\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 3.9634 - accuracy: 0.0915 - val_loss: 8.2024 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/150\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 3.6980 - accuracy: 0.1479 - val_loss: 8.3856 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/150\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 3.4075 - accuracy: 0.2042 - val_loss: 8.4796 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/150\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 3.1600 - accuracy: 0.2606 - val_loss: 8.5266 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/150\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 2.8915 - accuracy: 0.3873 - val_loss: 8.7070 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/150\n",
            "5/5 [==============================] - 0s 55ms/step - loss: 2.6814 - accuracy: 0.4437 - val_loss: 8.7263 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/150\n",
            "5/5 [==============================] - 0s 49ms/step - loss: 2.4835 - accuracy: 0.5282 - val_loss: 8.6639 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/150\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 2.3140 - accuracy: 0.5704 - val_loss: 8.8262 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/150\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 2.1486 - accuracy: 0.6549 - val_loss: 8.9294 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/150\n",
            "5/5 [==============================] - 0s 53ms/step - loss: 1.9872 - accuracy: 0.7113 - val_loss: 8.9801 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/150\n",
            "5/5 [==============================] - 0s 53ms/step - loss: 1.8546 - accuracy: 0.7465 - val_loss: 9.0978 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/150\n",
            "5/5 [==============================] - 0s 60ms/step - loss: 1.6933 - accuracy: 0.8239 - val_loss: 9.3007 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/150\n",
            "5/5 [==============================] - 0s 50ms/step - loss: 1.5885 - accuracy: 0.8592 - val_loss: 9.2101 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/150\n",
            "5/5 [==============================] - 0s 54ms/step - loss: 1.4801 - accuracy: 0.8592 - val_loss: 9.3124 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/150\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 1.3638 - accuracy: 0.8803 - val_loss: 9.3788 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/150\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 1.2742 - accuracy: 0.9437 - val_loss: 9.4448 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/150\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 1.1967 - accuracy: 0.9507 - val_loss: 9.5496 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/150\n",
            "5/5 [==============================] - 1s 123ms/step - loss: 1.0996 - accuracy: 0.9648 - val_loss: 9.5199 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/150\n",
            "5/5 [==============================] - 0s 55ms/step - loss: 1.0292 - accuracy: 0.9930 - val_loss: 9.5247 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/150\n",
            "5/5 [==============================] - 0s 53ms/step - loss: 0.9641 - accuracy: 0.9718 - val_loss: 9.5654 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/150\n",
            "5/5 [==============================] - 0s 55ms/step - loss: 0.9102 - accuracy: 0.9718 - val_loss: 9.7922 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/150\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.8490 - accuracy: 0.9859 - val_loss: 9.7508 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/150\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.7910 - accuracy: 0.9930 - val_loss: 9.8059 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/150\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.7420 - accuracy: 0.9859 - val_loss: 9.9127 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/150\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.6941 - accuracy: 0.9859 - val_loss: 9.9330 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/150\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.6533 - accuracy: 0.9789 - val_loss: 9.9839 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/150\n",
            "5/5 [==============================] - 1s 153ms/step - loss: 0.6257 - accuracy: 0.9859 - val_loss: 10.0350 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/150\n",
            "5/5 [==============================] - 1s 166ms/step - loss: 0.5901 - accuracy: 0.9859 - val_loss: 10.0798 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/150\n",
            "5/5 [==============================] - 1s 138ms/step - loss: 0.5619 - accuracy: 0.9859 - val_loss: 10.1636 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/150\n",
            "5/5 [==============================] - 1s 180ms/step - loss: 0.5246 - accuracy: 1.0000 - val_loss: 10.1224 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/150\n",
            "5/5 [==============================] - 1s 213ms/step - loss: 0.4998 - accuracy: 1.0000 - val_loss: 10.1610 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/150\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.4710 - accuracy: 1.0000 - val_loss: 10.2560 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/150\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.4428 - accuracy: 1.0000 - val_loss: 10.2927 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/150\n",
            "5/5 [==============================] - 1s 174ms/step - loss: 0.4275 - accuracy: 1.0000 - val_loss: 10.3264 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/150\n",
            "5/5 [==============================] - 1s 151ms/step - loss: 0.3987 - accuracy: 1.0000 - val_loss: 10.3826 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/150\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.3830 - accuracy: 1.0000 - val_loss: 10.3623 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/150\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 0.3657 - accuracy: 1.0000 - val_loss: 10.4551 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/150\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.3465 - accuracy: 1.0000 - val_loss: 10.4721 - val_accuracy: 0.0000e+00\n",
            "Epoch 51/150\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.3284 - accuracy: 1.0000 - val_loss: 10.4487 - val_accuracy: 0.0000e+00\n",
            "Epoch 52/150\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.3170 - accuracy: 1.0000 - val_loss: 10.5270 - val_accuracy: 0.0000e+00\n",
            "Epoch 53/150\n",
            "5/5 [==============================] - 0s 61ms/step - loss: 0.3031 - accuracy: 1.0000 - val_loss: 10.6006 - val_accuracy: 0.0000e+00\n",
            "Epoch 54/150\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 0.2903 - accuracy: 1.0000 - val_loss: 10.5960 - val_accuracy: 0.0000e+00\n",
            "Epoch 55/150\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.2735 - accuracy: 1.0000 - val_loss: 10.5585 - val_accuracy: 0.0000e+00\n",
            "Epoch 56/150\n",
            "5/5 [==============================] - 0s 60ms/step - loss: 0.2670 - accuracy: 1.0000 - val_loss: 10.5870 - val_accuracy: 0.0000e+00\n",
            "Epoch 57/150\n",
            "5/5 [==============================] - 0s 60ms/step - loss: 0.2561 - accuracy: 1.0000 - val_loss: 10.6167 - val_accuracy: 0.0000e+00\n",
            "Epoch 58/150\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 0.2459 - accuracy: 1.0000 - val_loss: 10.6943 - val_accuracy: 0.0000e+00\n",
            "Epoch 59/150\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 0.2347 - accuracy: 1.0000 - val_loss: 10.7374 - val_accuracy: 0.0000e+00\n",
            "Epoch 60/150\n",
            "5/5 [==============================] - 0s 53ms/step - loss: 0.2262 - accuracy: 1.0000 - val_loss: 10.7226 - val_accuracy: 0.0000e+00\n",
            "Epoch 61/150\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 0.2181 - accuracy: 1.0000 - val_loss: 10.7591 - val_accuracy: 0.0000e+00\n",
            "Epoch 62/150\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 0.2110 - accuracy: 1.0000 - val_loss: 10.7828 - val_accuracy: 0.0000e+00\n",
            "Epoch 63/150\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 0.2052 - accuracy: 1.0000 - val_loss: 10.7547 - val_accuracy: 0.0000e+00\n",
            "Epoch 64/150\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 0.1960 - accuracy: 1.0000 - val_loss: 10.7436 - val_accuracy: 0.0000e+00\n",
            "Epoch 65/150\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 0.1891 - accuracy: 1.0000 - val_loss: 10.7821 - val_accuracy: 0.0000e+00\n",
            "Epoch 66/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.1830 - accuracy: 1.0000 - val_loss: 10.8361 - val_accuracy: 0.0000e+00\n",
            "Epoch 67/150\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 0.1763 - accuracy: 1.0000 - val_loss: 10.8707 - val_accuracy: 0.0000e+00\n",
            "Epoch 68/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 0.1715 - accuracy: 1.0000 - val_loss: 10.9082 - val_accuracy: 0.0000e+00\n",
            "Epoch 69/150\n",
            "5/5 [==============================] - 0s 61ms/step - loss: 0.1658 - accuracy: 1.0000 - val_loss: 10.9203 - val_accuracy: 0.0000e+00\n",
            "Epoch 70/150\n",
            "5/5 [==============================] - 0s 49ms/step - loss: 0.1613 - accuracy: 1.0000 - val_loss: 10.8901 - val_accuracy: 0.0000e+00\n",
            "Epoch 71/150\n",
            "5/5 [==============================] - 0s 54ms/step - loss: 0.1575 - accuracy: 1.0000 - val_loss: 10.9041 - val_accuracy: 0.0000e+00\n",
            "Epoch 72/150\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 0.1513 - accuracy: 1.0000 - val_loss: 10.9160 - val_accuracy: 0.0000e+00\n",
            "Epoch 73/150\n",
            "5/5 [==============================] - 0s 55ms/step - loss: 0.1487 - accuracy: 1.0000 - val_loss: 10.9271 - val_accuracy: 0.0000e+00\n",
            "Epoch 74/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 0.1445 - accuracy: 1.0000 - val_loss: 10.9366 - val_accuracy: 0.0000e+00\n",
            "Epoch 75/150\n",
            "5/5 [==============================] - 0s 61ms/step - loss: 0.1387 - accuracy: 1.0000 - val_loss: 10.9778 - val_accuracy: 0.0000e+00\n",
            "Epoch 76/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 0.1364 - accuracy: 1.0000 - val_loss: 10.9962 - val_accuracy: 0.0000e+00\n",
            "Epoch 77/150\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.1311 - accuracy: 1.0000 - val_loss: 11.0479 - val_accuracy: 0.0000e+00\n",
            "Epoch 78/150\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.1276 - accuracy: 1.0000 - val_loss: 11.0651 - val_accuracy: 0.0000e+00\n",
            "Epoch 79/150\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.1254 - accuracy: 1.0000 - val_loss: 11.0579 - val_accuracy: 0.0000e+00\n",
            "Epoch 80/150\n",
            "5/5 [==============================] - 0s 104ms/step - loss: 0.1214 - accuracy: 1.0000 - val_loss: 11.0748 - val_accuracy: 0.0000e+00\n",
            "Epoch 81/150\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.1181 - accuracy: 1.0000 - val_loss: 11.0956 - val_accuracy: 0.0000e+00\n",
            "Epoch 82/150\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.1160 - accuracy: 1.0000 - val_loss: 11.1084 - val_accuracy: 0.0000e+00\n",
            "Epoch 83/150\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.1116 - accuracy: 1.0000 - val_loss: 11.1104 - val_accuracy: 0.0000e+00\n",
            "Epoch 84/150\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.1099 - accuracy: 1.0000 - val_loss: 11.1192 - val_accuracy: 0.0000e+00\n",
            "Epoch 85/150\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.1057 - accuracy: 1.0000 - val_loss: 11.1306 - val_accuracy: 0.0000e+00\n",
            "Epoch 86/150\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.1052 - accuracy: 1.0000 - val_loss: 11.1466 - val_accuracy: 0.0000e+00\n",
            "Epoch 87/150\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.1022 - accuracy: 1.0000 - val_loss: 11.1611 - val_accuracy: 0.0000e+00\n",
            "Epoch 88/150\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.0987 - accuracy: 1.0000 - val_loss: 11.1932 - val_accuracy: 0.0000e+00\n",
            "Epoch 89/150\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.0959 - accuracy: 1.0000 - val_loss: 11.2032 - val_accuracy: 0.0000e+00\n",
            "Epoch 90/150\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 0.0954 - accuracy: 1.0000 - val_loss: 11.2200 - val_accuracy: 0.0000e+00\n",
            "Epoch 91/150\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 0.0927 - accuracy: 1.0000 - val_loss: 11.2249 - val_accuracy: 0.0000e+00\n",
            "Epoch 92/150\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 0.0912 - accuracy: 1.0000 - val_loss: 11.2175 - val_accuracy: 0.0000e+00\n",
            "Epoch 93/150\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 0.0905 - accuracy: 1.0000 - val_loss: 11.2249 - val_accuracy: 0.0000e+00\n",
            "Epoch 94/150\n",
            "5/5 [==============================] - 0s 53ms/step - loss: 0.0881 - accuracy: 1.0000 - val_loss: 11.2594 - val_accuracy: 0.0000e+00\n",
            "Epoch 95/150\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 0.0856 - accuracy: 1.0000 - val_loss: 11.2769 - val_accuracy: 0.0000e+00\n",
            "Epoch 96/150\n",
            "5/5 [==============================] - 0s 61ms/step - loss: 0.0839 - accuracy: 1.0000 - val_loss: 11.2768 - val_accuracy: 0.0000e+00\n",
            "Epoch 97/150\n",
            "5/5 [==============================] - 0s 53ms/step - loss: 0.0823 - accuracy: 1.0000 - val_loss: 11.2873 - val_accuracy: 0.0000e+00\n",
            "Epoch 98/150\n",
            "5/5 [==============================] - 0s 53ms/step - loss: 0.0800 - accuracy: 1.0000 - val_loss: 11.3018 - val_accuracy: 0.0000e+00\n",
            "Epoch 99/150\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 0.0785 - accuracy: 1.0000 - val_loss: 11.3167 - val_accuracy: 0.0000e+00\n",
            "Epoch 100/150\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.0766 - accuracy: 1.0000 - val_loss: 11.3292 - val_accuracy: 0.0000e+00\n",
            "Epoch 101/150\n",
            "5/5 [==============================] - 0s 60ms/step - loss: 0.0761 - accuracy: 1.0000 - val_loss: 11.3520 - val_accuracy: 0.0000e+00\n",
            "Epoch 102/150\n",
            "5/5 [==============================] - 0s 50ms/step - loss: 0.0743 - accuracy: 1.0000 - val_loss: 11.3653 - val_accuracy: 0.0000e+00\n",
            "Epoch 103/150\n",
            "5/5 [==============================] - 0s 60ms/step - loss: 0.0721 - accuracy: 1.0000 - val_loss: 11.3704 - val_accuracy: 0.0000e+00\n",
            "Epoch 104/150\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0717 - accuracy: 1.0000 - val_loss: 11.3719 - val_accuracy: 0.0000e+00\n",
            "Epoch 105/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0707 - accuracy: 1.0000 - val_loss: 11.3726 - val_accuracy: 0.0000e+00\n",
            "Epoch 106/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 0.0681 - accuracy: 1.0000 - val_loss: 11.3893 - val_accuracy: 0.0000e+00\n",
            "Epoch 107/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0677 - accuracy: 1.0000 - val_loss: 11.4002 - val_accuracy: 0.0000e+00\n",
            "Epoch 108/150\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0659 - accuracy: 1.0000 - val_loss: 11.4377 - val_accuracy: 0.0000e+00\n",
            "Epoch 109/150\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 0.0644 - accuracy: 1.0000 - val_loss: 11.4636 - val_accuracy: 0.0000e+00\n",
            "Epoch 110/150\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 0.0646 - accuracy: 1.0000 - val_loss: 11.4551 - val_accuracy: 0.0000e+00\n",
            "Epoch 111/150\n",
            "5/5 [==============================] - 0s 61ms/step - loss: 0.0626 - accuracy: 1.0000 - val_loss: 11.4593 - val_accuracy: 0.0000e+00\n",
            "Epoch 112/150\n",
            "5/5 [==============================] - 0s 49ms/step - loss: 0.0615 - accuracy: 1.0000 - val_loss: 11.4659 - val_accuracy: 0.0000e+00\n",
            "Epoch 113/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 0.0606 - accuracy: 1.0000 - val_loss: 11.4567 - val_accuracy: 0.0000e+00\n",
            "Epoch 114/150\n",
            "5/5 [==============================] - 0s 53ms/step - loss: 0.0595 - accuracy: 1.0000 - val_loss: 11.4526 - val_accuracy: 0.0000e+00\n",
            "Epoch 115/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0582 - accuracy: 1.0000 - val_loss: 11.4710 - val_accuracy: 0.0000e+00\n",
            "Epoch 116/150\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.0569 - accuracy: 1.0000 - val_loss: 11.5001 - val_accuracy: 0.0000e+00\n",
            "Epoch 117/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0564 - accuracy: 1.0000 - val_loss: 11.5082 - val_accuracy: 0.0000e+00\n",
            "Epoch 118/150\n",
            "5/5 [==============================] - 0s 61ms/step - loss: 0.0559 - accuracy: 1.0000 - val_loss: 11.5030 - val_accuracy: 0.0000e+00\n",
            "Epoch 119/150\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 0.0551 - accuracy: 1.0000 - val_loss: 11.5208 - val_accuracy: 0.0000e+00\n",
            "Epoch 120/150\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0542 - accuracy: 1.0000 - val_loss: 11.5512 - val_accuracy: 0.0000e+00\n",
            "Epoch 121/150\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.0530 - accuracy: 1.0000 - val_loss: 11.5714 - val_accuracy: 0.0000e+00\n",
            "Epoch 122/150\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.0521 - accuracy: 1.0000 - val_loss: 11.5783 - val_accuracy: 0.0000e+00\n",
            "Epoch 123/150\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.0515 - accuracy: 1.0000 - val_loss: 11.5717 - val_accuracy: 0.0000e+00\n",
            "Epoch 124/150\n",
            "5/5 [==============================] - 1s 218ms/step - loss: 0.0504 - accuracy: 1.0000 - val_loss: 11.5648 - val_accuracy: 0.0000e+00\n",
            "Epoch 125/150\n",
            "5/5 [==============================] - 1s 151ms/step - loss: 0.0496 - accuracy: 1.0000 - val_loss: 11.5741 - val_accuracy: 0.0000e+00\n",
            "Epoch 126/150\n",
            "5/5 [==============================] - 1s 151ms/step - loss: 0.0492 - accuracy: 1.0000 - val_loss: 11.5896 - val_accuracy: 0.0000e+00\n",
            "Epoch 127/150\n",
            "5/5 [==============================] - 1s 159ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 11.5977 - val_accuracy: 0.0000e+00\n",
            "Epoch 128/150\n",
            "5/5 [==============================] - 1s 181ms/step - loss: 0.0477 - accuracy: 1.0000 - val_loss: 11.6097 - val_accuracy: 0.0000e+00\n",
            "Epoch 129/150\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.0470 - accuracy: 1.0000 - val_loss: 11.6247 - val_accuracy: 0.0000e+00\n",
            "Epoch 130/150\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.0461 - accuracy: 1.0000 - val_loss: 11.6504 - val_accuracy: 0.0000e+00\n",
            "Epoch 131/150\n",
            "5/5 [==============================] - 1s 156ms/step - loss: 0.0450 - accuracy: 1.0000 - val_loss: 11.6684 - val_accuracy: 0.0000e+00\n",
            "Epoch 132/150\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0450 - accuracy: 1.0000 - val_loss: 11.6749 - val_accuracy: 0.0000e+00\n",
            "Epoch 133/150\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.0441 - accuracy: 1.0000 - val_loss: 11.6770 - val_accuracy: 0.0000e+00\n",
            "Epoch 134/150\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.0436 - accuracy: 1.0000 - val_loss: 11.6710 - val_accuracy: 0.0000e+00\n",
            "Epoch 135/150\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 11.6623 - val_accuracy: 0.0000e+00\n",
            "Epoch 136/150\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.0425 - accuracy: 1.0000 - val_loss: 11.6600 - val_accuracy: 0.0000e+00\n",
            "Epoch 137/150\n",
            "5/5 [==============================] - 0s 105ms/step - loss: 0.0419 - accuracy: 1.0000 - val_loss: 11.6733 - val_accuracy: 0.0000e+00\n",
            "Epoch 138/150\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 0.0410 - accuracy: 1.0000 - val_loss: 11.6898 - val_accuracy: 0.0000e+00\n",
            "Epoch 139/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 11.7182 - val_accuracy: 0.0000e+00\n",
            "Epoch 140/150\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 0.0399 - accuracy: 1.0000 - val_loss: 11.7326 - val_accuracy: 0.0000e+00\n",
            "Epoch 141/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0397 - accuracy: 1.0000 - val_loss: 11.7298 - val_accuracy: 0.0000e+00\n",
            "Epoch 142/150\n",
            "5/5 [==============================] - 0s 49ms/step - loss: 0.0389 - accuracy: 1.0000 - val_loss: 11.7206 - val_accuracy: 0.0000e+00\n",
            "Epoch 143/150\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 0.0386 - accuracy: 1.0000 - val_loss: 11.7252 - val_accuracy: 0.0000e+00\n",
            "Epoch 144/150\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 11.7362 - val_accuracy: 0.0000e+00\n",
            "Epoch 145/150\n",
            "5/5 [==============================] - 0s 55ms/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 11.7339 - val_accuracy: 0.0000e+00\n",
            "Epoch 146/150\n",
            "5/5 [==============================] - 0s 53ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 11.7409 - val_accuracy: 0.0000e+00\n",
            "Epoch 147/150\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 11.7475 - val_accuracy: 0.0000e+00\n",
            "Epoch 148/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 11.7556 - val_accuracy: 0.0000e+00\n",
            "Epoch 149/150\n",
            "5/5 [==============================] - 0s 55ms/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 11.7657 - val_accuracy: 0.0000e+00\n",
            "Epoch 150/150\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 0.0352 - accuracy: 1.0000 - val_loss: 11.7822 - val_accuracy: 0.0000e+00\n",
            "training model2\n",
            "Epoch 1/150\n",
            "5/5 [==============================] - 13s 569ms/step - loss: 5.3189 - accuracy: 0.0141 - val_loss: 5.3215 - val_accuracy: 0.0105\n",
            "Epoch 2/150\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 5.3134 - accuracy: 0.0282 - val_loss: 5.3275 - val_accuracy: 0.0105\n",
            "Epoch 3/150\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 5.3068 - accuracy: 0.0282 - val_loss: 5.3388 - val_accuracy: 0.0105\n",
            "Epoch 4/150\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 5.2923 - accuracy: 0.0282 - val_loss: 5.3651 - val_accuracy: 0.0105\n",
            "Epoch 5/150\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 5.2585 - accuracy: 0.0282 - val_loss: 5.4533 - val_accuracy: 0.0105\n",
            "Epoch 6/150\n",
            "5/5 [==============================] - 0s 50ms/step - loss: 5.1666 - accuracy: 0.0282 - val_loss: 5.8263 - val_accuracy: 0.0105\n",
            "Epoch 7/150\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 5.0126 - accuracy: 0.0282 - val_loss: 6.8265 - val_accuracy: 0.0105\n",
            "Epoch 8/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 4.9541 - accuracy: 0.0282 - val_loss: 7.3525 - val_accuracy: 0.0105\n",
            "Epoch 9/150\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 4.9079 - accuracy: 0.0282 - val_loss: 7.4291 - val_accuracy: 0.0105\n",
            "Epoch 10/150\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 4.8695 - accuracy: 0.0282 - val_loss: 7.5305 - val_accuracy: 0.0105\n",
            "Epoch 11/150\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 4.8549 - accuracy: 0.0282 - val_loss: 7.6738 - val_accuracy: 0.0105\n",
            "Epoch 12/150\n",
            "5/5 [==============================] - 0s 55ms/step - loss: 4.8464 - accuracy: 0.0282 - val_loss: 7.8486 - val_accuracy: 0.0105\n",
            "Epoch 13/150\n",
            "5/5 [==============================] - 0s 50ms/step - loss: 4.8412 - accuracy: 0.0282 - val_loss: 8.0453 - val_accuracy: 0.0105\n",
            "Epoch 14/150\n",
            "5/5 [==============================] - 0s 55ms/step - loss: 4.8360 - accuracy: 0.0282 - val_loss: 8.2956 - val_accuracy: 0.0105\n",
            "Epoch 15/150\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 4.8326 - accuracy: 0.0282 - val_loss: 8.4852 - val_accuracy: 0.0105\n",
            "Epoch 16/150\n",
            "5/5 [==============================] - 0s 53ms/step - loss: 4.8301 - accuracy: 0.0282 - val_loss: 8.6296 - val_accuracy: 0.0105\n",
            "Epoch 17/150\n",
            "5/5 [==============================] - 0s 50ms/step - loss: 4.8309 - accuracy: 0.0282 - val_loss: 8.7044 - val_accuracy: 0.0105\n",
            "Epoch 18/150\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 4.8265 - accuracy: 0.0282 - val_loss: 8.7483 - val_accuracy: 0.0105\n",
            "Epoch 19/150\n",
            "5/5 [==============================] - 0s 48ms/step - loss: 4.8236 - accuracy: 0.0282 - val_loss: 8.8259 - val_accuracy: 0.0105\n",
            "Epoch 20/150\n",
            "5/5 [==============================] - 0s 60ms/step - loss: 4.8251 - accuracy: 0.0282 - val_loss: 8.9120 - val_accuracy: 0.0105\n",
            "Epoch 21/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 4.8248 - accuracy: 0.0282 - val_loss: 9.0433 - val_accuracy: 0.0105\n",
            "Epoch 22/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 4.8237 - accuracy: 0.0282 - val_loss: 9.0805 - val_accuracy: 0.0105\n",
            "Epoch 23/150\n",
            "5/5 [==============================] - 0s 60ms/step - loss: 4.8241 - accuracy: 0.0282 - val_loss: 9.1275 - val_accuracy: 0.0105\n",
            "Epoch 24/150\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 4.8248 - accuracy: 0.0282 - val_loss: 9.1831 - val_accuracy: 0.0105\n",
            "Epoch 25/150\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 4.8239 - accuracy: 0.0282 - val_loss: 9.2112 - val_accuracy: 0.0105\n",
            "Epoch 26/150\n",
            "5/5 [==============================] - 0s 49ms/step - loss: 4.8218 - accuracy: 0.0282 - val_loss: 9.2150 - val_accuracy: 0.0105\n",
            "Epoch 27/150\n",
            "5/5 [==============================] - 0s 50ms/step - loss: 4.8224 - accuracy: 0.0282 - val_loss: 9.2014 - val_accuracy: 0.0105\n",
            "Epoch 28/150\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 4.8202 - accuracy: 0.0282 - val_loss: 9.2161 - val_accuracy: 0.0105\n",
            "Epoch 29/150\n",
            "5/5 [==============================] - 0s 55ms/step - loss: 4.8190 - accuracy: 0.0282 - val_loss: 9.2211 - val_accuracy: 0.0105\n",
            "Epoch 30/150\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 4.8208 - accuracy: 0.0282 - val_loss: 9.2464 - val_accuracy: 0.0105\n",
            "Epoch 31/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 4.8193 - accuracy: 0.0282 - val_loss: 9.2938 - val_accuracy: 0.0105\n",
            "Epoch 32/150\n",
            "5/5 [==============================] - 0s 60ms/step - loss: 4.8217 - accuracy: 0.0282 - val_loss: 9.3247 - val_accuracy: 0.0105\n",
            "Epoch 33/150\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 4.8190 - accuracy: 0.0282 - val_loss: 9.3749 - val_accuracy: 0.0105\n",
            "Epoch 34/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 4.8183 - accuracy: 0.0282 - val_loss: 9.3864 - val_accuracy: 0.0105\n",
            "Epoch 35/150\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 4.8194 - accuracy: 0.0282 - val_loss: 9.4198 - val_accuracy: 0.0105\n",
            "Epoch 36/150\n",
            "5/5 [==============================] - 0s 60ms/step - loss: 4.8191 - accuracy: 0.0282 - val_loss: 9.4667 - val_accuracy: 0.0105\n",
            "Epoch 37/150\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 4.8199 - accuracy: 0.0282 - val_loss: 9.4502 - val_accuracy: 0.0105\n",
            "Epoch 38/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 4.8179 - accuracy: 0.0282 - val_loss: 9.3975 - val_accuracy: 0.0105\n",
            "Epoch 39/150\n",
            "5/5 [==============================] - 0s 54ms/step - loss: 4.8193 - accuracy: 0.0282 - val_loss: 9.3971 - val_accuracy: 0.0105\n",
            "Epoch 40/150\n",
            "5/5 [==============================] - 0s 49ms/step - loss: 4.8173 - accuracy: 0.0282 - val_loss: 9.3960 - val_accuracy: 0.0105\n",
            "Epoch 41/150\n",
            "5/5 [==============================] - 0s 50ms/step - loss: 4.8184 - accuracy: 0.0282 - val_loss: 9.4531 - val_accuracy: 0.0105\n",
            "Epoch 42/150\n",
            "5/5 [==============================] - 0s 60ms/step - loss: 4.8194 - accuracy: 0.0282 - val_loss: 9.5240 - val_accuracy: 0.0105\n",
            "Epoch 43/150\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 4.8179 - accuracy: 0.0282 - val_loss: 9.5440 - val_accuracy: 0.0105\n",
            "Epoch 44/150\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 4.8180 - accuracy: 0.0282 - val_loss: 9.5642 - val_accuracy: 0.0105\n",
            "Epoch 45/150\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 4.8176 - accuracy: 0.0282 - val_loss: 9.5445 - val_accuracy: 0.0105\n",
            "Epoch 46/150\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 4.8185 - accuracy: 0.0282 - val_loss: 9.5643 - val_accuracy: 0.0105\n",
            "Epoch 47/150\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 4.8168 - accuracy: 0.0282 - val_loss: 9.4894 - val_accuracy: 0.0105\n",
            "Epoch 48/150\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 4.8172 - accuracy: 0.0282 - val_loss: 9.4736 - val_accuracy: 0.0105\n",
            "Epoch 49/150\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 4.8161 - accuracy: 0.0282 - val_loss: 9.5573 - val_accuracy: 0.0105\n",
            "Epoch 50/150\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 4.8150 - accuracy: 0.0282 - val_loss: 9.5648 - val_accuracy: 0.0105\n",
            "Epoch 51/150\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 4.8170 - accuracy: 0.0282 - val_loss: 9.6035 - val_accuracy: 0.0105\n",
            "Epoch 52/150\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 4.8165 - accuracy: 0.0282 - val_loss: 9.6786 - val_accuracy: 0.0105\n",
            "Epoch 53/150\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 4.8148 - accuracy: 0.0282 - val_loss: 9.7753 - val_accuracy: 0.0105\n",
            "Epoch 54/150\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 4.8166 - accuracy: 0.0282 - val_loss: 9.8267 - val_accuracy: 0.0105\n",
            "Epoch 55/150\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 4.8153 - accuracy: 0.0282 - val_loss: 9.8447 - val_accuracy: 0.0105\n",
            "Epoch 56/150\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 4.8169 - accuracy: 0.0282 - val_loss: 9.8611 - val_accuracy: 0.0105\n",
            "Epoch 57/150\n",
            "5/5 [==============================] - 1s 157ms/step - loss: 4.8162 - accuracy: 0.0282 - val_loss: 9.7907 - val_accuracy: 0.0105\n",
            "Epoch 58/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 4.8163 - accuracy: 0.0282 - val_loss: 9.8016 - val_accuracy: 0.0105\n",
            "Epoch 59/150\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 4.8160 - accuracy: 0.0282 - val_loss: 9.8219 - val_accuracy: 0.0105\n",
            "Epoch 60/150\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 4.8151 - accuracy: 0.0282 - val_loss: 9.7792 - val_accuracy: 0.0105\n",
            "Epoch 61/150\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 4.8144 - accuracy: 0.0282 - val_loss: 9.7909 - val_accuracy: 0.0105\n",
            "Epoch 62/150\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 4.8152 - accuracy: 0.0282 - val_loss: 9.7984 - val_accuracy: 0.0105\n",
            "Epoch 63/150\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 4.8141 - accuracy: 0.0282 - val_loss: 9.8148 - val_accuracy: 0.0105\n",
            "Epoch 64/150\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 4.8160 - accuracy: 0.0282 - val_loss: 9.8047 - val_accuracy: 0.0105\n",
            "Epoch 65/150\n",
            "5/5 [==============================] - 1s 105ms/step - loss: 4.8150 - accuracy: 0.0282 - val_loss: 9.7530 - val_accuracy: 0.0105\n",
            "Epoch 66/150\n",
            "5/5 [==============================] - 1s 113ms/step - loss: 4.8148 - accuracy: 0.0282 - val_loss: 9.7377 - val_accuracy: 0.0105\n",
            "Epoch 67/150\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 4.8153 - accuracy: 0.0282 - val_loss: 9.8148 - val_accuracy: 0.0105\n",
            "Epoch 68/150\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 4.8141 - accuracy: 0.0282 - val_loss: 9.8826 - val_accuracy: 0.0105\n",
            "Epoch 69/150\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 4.8155 - accuracy: 0.0282 - val_loss: 9.8414 - val_accuracy: 0.0105\n",
            "Epoch 70/150\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 4.8139 - accuracy: 0.0282 - val_loss: 9.8857 - val_accuracy: 0.0105\n",
            "Epoch 71/150\n",
            "5/5 [==============================] - 0s 108ms/step - loss: 4.8152 - accuracy: 0.0282 - val_loss: 9.9822 - val_accuracy: 0.0105\n",
            "Epoch 72/150\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 4.8146 - accuracy: 0.0282 - val_loss: 10.0017 - val_accuracy: 0.0105\n",
            "Epoch 73/150\n",
            "5/5 [==============================] - 1s 124ms/step - loss: 4.8143 - accuracy: 0.0282 - val_loss: 9.9252 - val_accuracy: 0.0105\n",
            "Epoch 74/150\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 4.8132 - accuracy: 0.0282 - val_loss: 9.8394 - val_accuracy: 0.0105\n",
            "Epoch 75/150\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 4.8147 - accuracy: 0.0282 - val_loss: 9.8641 - val_accuracy: 0.0105\n",
            "Epoch 76/150\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 4.8149 - accuracy: 0.0282 - val_loss: 9.8247 - val_accuracy: 0.0105\n",
            "Epoch 77/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 4.8138 - accuracy: 0.0282 - val_loss: 9.7697 - val_accuracy: 0.0105\n",
            "Epoch 78/150\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 4.8156 - accuracy: 0.0282 - val_loss: 9.7835 - val_accuracy: 0.0105\n",
            "Epoch 79/150\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 4.8148 - accuracy: 0.0282 - val_loss: 9.7812 - val_accuracy: 0.0105\n",
            "Epoch 80/150\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 4.8135 - accuracy: 0.0282 - val_loss: 9.8180 - val_accuracy: 0.0105\n",
            "Epoch 81/150\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 4.8143 - accuracy: 0.0282 - val_loss: 9.8557 - val_accuracy: 0.0105\n",
            "Epoch 82/150\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 4.8131 - accuracy: 0.0282 - val_loss: 10.0830 - val_accuracy: 0.0105\n",
            "Epoch 83/150\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 4.8149 - accuracy: 0.0282 - val_loss: 10.2126 - val_accuracy: 0.0105\n",
            "Epoch 84/150\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 4.8158 - accuracy: 0.0282 - val_loss: 10.2373 - val_accuracy: 0.0105\n",
            "Epoch 85/150\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 4.8168 - accuracy: 0.0282 - val_loss: 10.1958 - val_accuracy: 0.0105\n",
            "Epoch 86/150\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 4.8153 - accuracy: 0.0282 - val_loss: 10.1406 - val_accuracy: 0.0105\n",
            "Epoch 87/150\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 4.8143 - accuracy: 0.0282 - val_loss: 10.0180 - val_accuracy: 0.0105\n",
            "Epoch 88/150\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 4.8146 - accuracy: 0.0282 - val_loss: 9.8892 - val_accuracy: 0.0105\n",
            "Epoch 89/150\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 4.8141 - accuracy: 0.0282 - val_loss: 9.7946 - val_accuracy: 0.0105\n",
            "Epoch 90/150\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 4.8147 - accuracy: 0.0282 - val_loss: 9.6798 - val_accuracy: 0.0105\n",
            "Epoch 91/150\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 4.8136 - accuracy: 0.0282 - val_loss: 9.5315 - val_accuracy: 0.0105\n",
            "Epoch 92/150\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 4.8142 - accuracy: 0.0282 - val_loss: 9.5517 - val_accuracy: 0.0105\n",
            "Epoch 93/150\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 4.8140 - accuracy: 0.0282 - val_loss: 9.6336 - val_accuracy: 0.0105\n",
            "Epoch 94/150\n",
            "5/5 [==============================] - 0s 50ms/step - loss: 4.8135 - accuracy: 0.0282 - val_loss: 9.7859 - val_accuracy: 0.0105\n",
            "Epoch 95/150\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 4.8130 - accuracy: 0.0282 - val_loss: 10.0239 - val_accuracy: 0.0105\n",
            "Epoch 96/150\n",
            "5/5 [==============================] - 0s 54ms/step - loss: 4.8140 - accuracy: 0.0282 - val_loss: 10.2567 - val_accuracy: 0.0105\n",
            "Epoch 97/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 4.8128 - accuracy: 0.0282 - val_loss: 10.3163 - val_accuracy: 0.0105\n",
            "Epoch 98/150\n",
            "5/5 [==============================] - 0s 49ms/step - loss: 4.8134 - accuracy: 0.0282 - val_loss: 10.2695 - val_accuracy: 0.0105\n",
            "Epoch 99/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 4.8128 - accuracy: 0.0282 - val_loss: 10.1889 - val_accuracy: 0.0105\n",
            "Epoch 100/150\n",
            "5/5 [==============================] - 0s 53ms/step - loss: 4.8124 - accuracy: 0.0282 - val_loss: 10.1571 - val_accuracy: 0.0105\n",
            "Epoch 101/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 4.8123 - accuracy: 0.0282 - val_loss: 10.1182 - val_accuracy: 0.0105\n",
            "Epoch 102/150\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 4.8126 - accuracy: 0.0282 - val_loss: 10.0685 - val_accuracy: 0.0105\n",
            "Epoch 103/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 4.8124 - accuracy: 0.0282 - val_loss: 10.1333 - val_accuracy: 0.0105\n",
            "Epoch 104/150\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 4.8143 - accuracy: 0.0282 - val_loss: 10.1417 - val_accuracy: 0.0105\n",
            "Epoch 105/150\n",
            "5/5 [==============================] - 0s 55ms/step - loss: 4.8134 - accuracy: 0.0282 - val_loss: 10.0940 - val_accuracy: 0.0105\n",
            "Epoch 106/150\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 4.8123 - accuracy: 0.0282 - val_loss: 9.9848 - val_accuracy: 0.0105\n",
            "Epoch 107/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 4.8132 - accuracy: 0.0282 - val_loss: 9.8976 - val_accuracy: 0.0105\n",
            "Epoch 108/150\n",
            "5/5 [==============================] - 0s 56ms/step - loss: 4.8133 - accuracy: 0.0282 - val_loss: 9.9321 - val_accuracy: 0.0105\n",
            "Epoch 109/150\n",
            "5/5 [==============================] - 0s 49ms/step - loss: 4.8131 - accuracy: 0.0282 - val_loss: 10.0590 - val_accuracy: 0.0105\n",
            "Epoch 110/150\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 4.8122 - accuracy: 0.0282 - val_loss: 10.1675 - val_accuracy: 0.0105\n",
            "Epoch 111/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 4.8129 - accuracy: 0.0282 - val_loss: 10.2971 - val_accuracy: 0.0105\n",
            "Epoch 112/150\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 4.8129 - accuracy: 0.0282 - val_loss: 10.3042 - val_accuracy: 0.0105\n",
            "Epoch 113/150\n",
            "5/5 [==============================] - 0s 54ms/step - loss: 4.8147 - accuracy: 0.0282 - val_loss: 10.4669 - val_accuracy: 0.0105\n",
            "Epoch 114/150\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 4.8130 - accuracy: 0.0282 - val_loss: 10.4737 - val_accuracy: 0.0105\n",
            "Epoch 115/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 4.8134 - accuracy: 0.0282 - val_loss: 10.4848 - val_accuracy: 0.0105\n",
            "Epoch 116/150\n",
            "5/5 [==============================] - 0s 50ms/step - loss: 4.8124 - accuracy: 0.0282 - val_loss: 10.3958 - val_accuracy: 0.0105\n",
            "Epoch 117/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 4.8131 - accuracy: 0.0282 - val_loss: 10.3143 - val_accuracy: 0.0105\n",
            "Epoch 118/150\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 4.8135 - accuracy: 0.0282 - val_loss: 10.2209 - val_accuracy: 0.0105\n",
            "Epoch 119/150\n",
            "5/5 [==============================] - 0s 55ms/step - loss: 4.8123 - accuracy: 0.0282 - val_loss: 10.1335 - val_accuracy: 0.0105\n",
            "Epoch 120/150\n",
            "5/5 [==============================] - 0s 61ms/step - loss: 4.8139 - accuracy: 0.0282 - val_loss: 9.9966 - val_accuracy: 0.0105\n",
            "Epoch 121/150\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 4.8136 - accuracy: 0.0282 - val_loss: 9.9352 - val_accuracy: 0.0105\n",
            "Epoch 122/150\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 4.8133 - accuracy: 0.0282 - val_loss: 9.8927 - val_accuracy: 0.0105\n",
            "Epoch 123/150\n",
            "5/5 [==============================] - 0s 55ms/step - loss: 4.8128 - accuracy: 0.0282 - val_loss: 9.9029 - val_accuracy: 0.0105\n",
            "Epoch 124/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 4.8140 - accuracy: 0.0282 - val_loss: 10.1066 - val_accuracy: 0.0105\n",
            "Epoch 125/150\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 4.8132 - accuracy: 0.0282 - val_loss: 10.1172 - val_accuracy: 0.0105\n",
            "Epoch 126/150\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 4.8121 - accuracy: 0.0282 - val_loss: 10.0025 - val_accuracy: 0.0105\n",
            "Epoch 127/150\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 4.8117 - accuracy: 0.0282 - val_loss: 10.0657 - val_accuracy: 0.0105\n",
            "Epoch 128/150\n",
            "5/5 [==============================] - 0s 55ms/step - loss: 4.8120 - accuracy: 0.0282 - val_loss: 10.1505 - val_accuracy: 0.0105\n",
            "Epoch 129/150\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 4.8127 - accuracy: 0.0282 - val_loss: 10.2932 - val_accuracy: 0.0105\n",
            "Epoch 130/150\n",
            "5/5 [==============================] - 0s 60ms/step - loss: 4.8119 - accuracy: 0.0282 - val_loss: 10.2536 - val_accuracy: 0.0105\n",
            "Epoch 131/150\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 4.8129 - accuracy: 0.0282 - val_loss: 10.0616 - val_accuracy: 0.0105\n",
            "Epoch 132/150\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 4.8121 - accuracy: 0.0282 - val_loss: 9.8353 - val_accuracy: 0.0105\n",
            "Epoch 133/150\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 4.8120 - accuracy: 0.0282 - val_loss: 9.8159 - val_accuracy: 0.0105\n",
            "Epoch 134/150\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 4.8119 - accuracy: 0.0282 - val_loss: 9.8662 - val_accuracy: 0.0105\n",
            "Epoch 135/150\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 4.8136 - accuracy: 0.0282 - val_loss: 9.7617 - val_accuracy: 0.0105\n",
            "Epoch 136/150\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 4.8117 - accuracy: 0.0282 - val_loss: 10.0948 - val_accuracy: 0.0105\n",
            "Epoch 137/150\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 4.8112 - accuracy: 0.0282 - val_loss: 10.3244 - val_accuracy: 0.0105\n",
            "Epoch 138/150\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 4.8131 - accuracy: 0.0282 - val_loss: 10.4560 - val_accuracy: 0.0105\n",
            "Epoch 139/150\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 4.8126 - accuracy: 0.0282 - val_loss: 10.5160 - val_accuracy: 0.0105\n",
            "Epoch 140/150\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 4.8125 - accuracy: 0.0282 - val_loss: 10.5236 - val_accuracy: 0.0105\n",
            "Epoch 141/150\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 4.8129 - accuracy: 0.0282 - val_loss: 10.4670 - val_accuracy: 0.0105\n",
            "Epoch 142/150\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 4.8135 - accuracy: 0.0282 - val_loss: 10.4804 - val_accuracy: 0.0105\n",
            "Epoch 143/150\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 4.8127 - accuracy: 0.0282 - val_loss: 10.4661 - val_accuracy: 0.0105\n",
            "Epoch 144/150\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 4.8125 - accuracy: 0.0282 - val_loss: 10.3678 - val_accuracy: 0.0105\n",
            "Epoch 145/150\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 4.8115 - accuracy: 0.0282 - val_loss: 10.2428 - val_accuracy: 0.0105\n",
            "Epoch 146/150\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 4.8118 - accuracy: 0.0282 - val_loss: 10.1856 - val_accuracy: 0.0105\n",
            "Epoch 147/150\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 4.8112 - accuracy: 0.0282 - val_loss: 10.1518 - val_accuracy: 0.0105\n",
            "Epoch 148/150\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 4.8112 - accuracy: 0.0282 - val_loss: 10.0501 - val_accuracy: 0.0105\n",
            "Epoch 149/150\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 4.8115 - accuracy: 0.0282 - val_loss: 10.1201 - val_accuracy: 0.0105\n",
            "Epoch 150/150\n",
            "5/5 [==============================] - 1s 123ms/step - loss: 4.8108 - accuracy: 0.0282 - val_loss: 10.1302 - val_accuracy: 0.0105\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "input_sequences = []\n",
        "for line in cleaned_text.split('.'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=6, padding='pre'))\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = keras.utils.to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "print('training model1')\n",
        "\n",
        "# Training Model 1\n",
        "history1 = model1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=150, verbose=1)\n",
        "\n",
        "print('training model2')\n",
        "\n",
        "# Training Model 2\n",
        "history2 = model2.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=150, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL TESTING"
      ],
      "metadata": {
        "id": "UCzeaxAtPfPP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79cddef6",
        "outputId": "923cf053-480f-4a98-c1ed-7115bb13cacb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss for Model1: 11.782182693481445\n",
            "Validation Loss for Model2: 10.130215644836426\n"
          ]
        }
      ],
      "source": [
        "val_loss_model1 = history1.history['val_loss'][-1]\n",
        "val_loss_model2 = history2.history['val_loss'][-1]\n",
        "\n",
        "print(f\"Validation Loss for Model1: {val_loss_model1}\")\n",
        "print(f\"Validation Loss for Model2: {val_loss_model2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a7969ec",
        "outputId": "8a13f82a-807b-4772-8a02-79927fb36261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the best model is best_performing_model2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "if val_loss_model1 < val_loss_model2:\n",
        "    best_model = model1\n",
        "    best_model_name = \"best_performing_model1.h5\"\n",
        "else:\n",
        "    best_model = model2\n",
        "    best_model_name = \"best_performing_model2.h5\"\n",
        "\n",
        "best_model.save(best_model_name)\n",
        "print(f\"the best model is {best_model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nJfj4R2_bSxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c703656"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the previously saved model\n",
        "model = load_model('best_performing_model2.h5')\n",
        "\n",
        "def predict_next_word(model, tokenizer, text, num_words=1):\n",
        "    \"\"\"\n",
        "    Predict the next set of words using the trained model.\n",
        "\n",
        "    Args:\n",
        "    - model (keras.Model): The trained model.\n",
        "    - tokenizer (Tokenizer): The tokenizer object used for preprocessing.\n",
        "    - text (str): The input text.\n",
        "    - num_words (int): The number of words to predict.\n",
        "\n",
        "    Returns:\n",
        "    - str: The predicted words.\n",
        "    \"\"\"\n",
        "    for _ in range(num_words):\n",
        "        # Tokenize and pad the text\n",
        "        sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=5, padding='pre')\n",
        "\n",
        "        # Predict the next word\n",
        "        predicted_probs = model.predict(sequence, verbose=0)\n",
        "        predicted = np.argmax(predicted_probs, axis=-1)\n",
        "\n",
        "        # Convert the predicted word index to a word\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        # Append the predicted word to the text\n",
        "        text += \" \" + output_word\n",
        "\n",
        "    return ' '.join(text.split(' ')[-num_words:])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prompt the user for input\n",
        "\n",
        "\n",
        "# Predict the next words\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "\n",
        "  text = input(\"Nyora manzwi mashanu: \")\n",
        "\n",
        "  if text == \"0\":\n",
        "      print(\"chiitiko ichi chaperera pano .....\")\n",
        "      break\n",
        "\n",
        "  else:\n",
        "   try:\n",
        "    user_input = text.split(\" \")\n",
        "    user_input = text[-5:]\n",
        "    print(user_input)\n",
        "\n",
        "    # Prompt the user for input\n",
        "    predicted_word = predict_next_word(model, tokenizer, user_input, num_words=1)\n",
        "    # Predict the next word\n",
        "    print(f\"inzwi rawanikwa rinoteera manzwi amaisa ndeiro: {predicted_word}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   except Exception as e:\n",
        "    print(\"Error: \", e)\n",
        "    continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx7NZs1rG9gq",
        "outputId": "32f98965-a3ff-4bca-871e-4ad06d974959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nyora manzwi mashanu: manga chena inoparira parere nhema\n",
            "nhema\n",
            "inzwi rawanikwa rinoteera manzwi amaisa ndeiro: nhamo\n",
            "Nyora manzwi mashanu: 0\n",
            "chiitiko ichi chaperera pano .....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iZKjiX5BaIpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MuEaQ25eKMbS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}