{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV4guIM_Y-Cu",
        "outputId": "37c64b45-8cdb-4de3-a0df-4af47a021f63"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "N-ie9eEPfgNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd0ccba4-0a61-4e3f-ce6e-461321952aca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "O8I9UlwlgDia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e931b9dd-f29e-4c53-ecf1-9fc782a57a40"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/text_shona.csv\",\"r\")\n",
        "text_shona = file.read()"
      ],
      "metadata": {
        "id": "U9hoFh8Gd7j7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_shona"
      ],
      "metadata": {
        "id": "h1MxGZaveMfz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a587c29c-c82b-45fb-e2e8-007fb55f2bbf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Mudunhu rekwaChitororo yaiva burushashike vanhu vachifarira muroora mutsva anga awuya mumusha wekwaNhoto. Ivo vaNhoto vainzwa manyuku nyuku nekuti mwanakomana wavo mukuru Wayne anga avaunzira ndangaruvazhe. Wayne anga avakushanda mudhorobha riripedyo nedunhu ravo. Kubva paakapedza zvidzidzo zvake zveOlevel akabva angoenda kudhorobha kunoshava. Akabva awana basa kwaPower Sales. Ikoko kudhorobha ndiko kwaakasangana netsvarakadenga yake yainzi Rosemary Chuma. Vakafambidzana mwedzi yakati kuti kusvika vaviri ava vapanana pamuviri. Vakabva vatoronga zvekutizisana. MaChuma akabva atora vana tete vake vaviri vakava ndivo vakamuperedza. Pavakasvika mudunhu raChitororo vakambogara pamba pabamunini vaWayne mazuva matatu muroora asati aperekwa mumusha memurume wake. Zuva rekuperekwa kwake rakazosvika. Munhu wese mudunhu iri anga akarimirira zuva iri. Dunhu rese rakaturunuka kuzoona svusvurandadya yanga yaunzwa naWayne mumusha wekwaNhoto. Musikana anga akaumbwa hake iyeye. Ndivo vanonzi Mwari vakatora nguva yavo vachinatsosika. Hapana murume ayisayemura kuumbwa kwaRosemary. Anga ariruva chairo sezita rake. Asi Wayne ndiye anga ayita rombo rakanaka kuvarume vese kuva netsvarakadenga iyi. Mari dzaikandwa kani munhu wese achingorumbidza mhando yemudzimai yanga yaunzwa naWayne. Vakaimba vakomana nevasikana muroora achifambiwa naye kubva kwabamunini baba Stern kuenda kwavatezvara vake. Wayingoti fambe fambe obva amira mari ichikandwa. Pagedhi muroora ndipo paakambonetsa chose achiramba kufamba kuratidza kuti mari yaikandwa yaiva shoma. Vakatanga vachamutukirira kusvika vapererwa nezano. Vatezvara ndivo vakazobudisa chitsama chemari achibva afamba kunosvika padoor. Apo pakamboita nyaya kusvika vanhu vazoudza Wayne kuti aburitse Mari. Iye Wayne wacho ayinzwa kuzvirova dundundu nekuti munhu wese akanga amboona Rosemary ayingomurumbidza. Akapindwa naye muroora mumba mari ikabiswa kuti jira rakamufukidza ribiswe kumeso. Richibiswa kudayi munhu wese akamira hoo nerunako rwaiva namaChuma. Madzimai akaridza mipururu nekutamba.Mbuya Rukondo vakabva vaita kurova zvavo chikwee vachisimuka pavanga vakagara. \"\"Haiwawo Wayne muzukuru wangu hapana zvawaita apa. Kutiunzira mapendeka emumadhorobha menyu umu. Guyu kutsvukira mukati mune masvosve. Hezvino ndiripano, wakasiya ini bhogosvogodo yako uchida njuzu yemumasuweji iyi?\"\" Vanhu vakati bvuu kuseka. Vanhu vese vakabva vatanga kunyomba muroora. Iyi itsika yagara ichiitwa kuti kana muroora achiperekwa anombonyombewa. Vamwe vanonyombewa kusvika vasvimha misodzi. Asi maChuma ayiratidza kuti ane moyo wakasindimara. Anga akaita kuburitsa ziso rakachena kuti wekee semukaka achitarisa munhu wese ayivemo. Mumwe mukadzi akabva arudinhura rumbo runonzi \"\"Chimhuka kwindi\"\" Mwana wenyu chimhukakwindi..... Heheheee Chimhukakwindi.... Huya uwone chimhukakwindi.... Heheee chimhukakwindi..... Vanhu vakasimuka kwakutanga kuimba nekutamba. Mamwe madzimai ayiita kutamba achiita kumuzunzira mazakwatira. Iye maChuma wacho kana kugaya zvake. Ayitoratidza kuti ayinakidzwa nazvo.\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text\n",
        "def clean_text(text_shona):\n",
        "    # Convert text to lowercase\n",
        "    text_shona = text_shona.lower()\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text_shona = re.sub(r'[^\\w\\s]', '', text_shona)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text_shona)\n",
        "\n",
        "    # Remove stopwords\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Join the words back into a cleaned text\n",
        "    cleaned_text = ' '.join(words)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "cleaned_text = clean_text(text_shona)"
      ],
      "metadata": {
        "id": "JqKgI6tsfQEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1df335ef-3dd4-438b-a8af-2d8fae23e040"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cleaned_text)"
      ],
      "metadata": {
        "id": "rrBkAbd2gxuC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb14744e-b993-45f6-bfb8-2d06267f7998"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mudunhu rekwachitororo yaiva burushashike vanhu vachifarira muroora mutsva anga awuya mumusha wekwanhoto ivo vanhoto vainzwa manyuku nyuku nekuti mwanakomana wavo mukuru wayne anga avaunzira ndangaruvazhe wayne anga avakushanda mudhorobha riripedyo nedunhu ravo kubva paakapedza zvidzidzo zvake zveolevel akabva angoenda kudhorobha kunoshava akabva awana basa kwapower sales ikoko kudhorobha ndiko kwaakasangana netsvarakadenga yake yainzi rosemary chuma vakafambidzana mwedzi yakati kuti kusvika vaviri ava vapanana pamuviri vakabva vatoronga zvekutizisana machuma akabva atora vana tete vake vaviri vakava ndivo vakamuperedza pavakasvika mudunhu rachitororo vakambogara pamba pabamunini vawayne mazuva matatu muroora asati aperekwa mumusha memurume wake zuva rekuperekwa kwake rakazosvika munhu wese mudunhu iri anga akarimirira zuva iri dunhu rese rakaturunuka kuzoona svusvurandadya yanga yaunzwa nawayne mumusha wekwanhoto musikana anga akaumbwa hake iyeye ndivo vanonzi mwari vakatora nguva yavo vachinatsosika hapana murume ayisayemura kuumbwa kwarosemary anga ariruva chairo sezita rake asi wayne ndiye anga ayita rombo rakanaka kuvarume vese kuva netsvarakadenga iyi mari dzaikandwa kani munhu wese achingorumbidza mhando yemudzimai yanga yaunzwa nawayne vakaimba vakomana nevasikana muroora achifambiwa naye kubva kwabamunini baba stern kuenda kwavatezvara vake wayingoti fambe fambe obva amira mari ichikandwa pagedhi muroora ndipo paakambonetsa chose achiramba kufamba kuratidza kuti mari yaikandwa yaiva shoma vakatanga vachamutukirira kusvika vapererwa nezano vatezvara ndivo vakazobudisa chitsama chemari achibva afamba kunosvika padoor apo pakamboita nyaya kusvika vanhu vazoudza wayne kuti aburitse mari iye wayne wacho ayinzwa kuzvirova dundundu nekuti munhu wese akanga amboona rosemary ayingomurumbidza akapindwa naye muroora mumba mari ikabiswa kuti jira rakamufukidza ribiswe kumeso richibiswa kudayi munhu wese akamira hoo nerunako rwaiva namachuma madzimai akaridza mipururu nekutambambuya rukondo vakabva vaita kurova zvavo chikwee vachisimuka pavanga vakagara haiwawo wayne muzukuru wangu hapana zvawaita apa kutiunzira mapendeka emumadhorobha menyu umu guyu kutsvukira mukati mune masvosve hezvino ndiripano wakasiya ini bhogosvogodo yako uchida njuzu yemumasuweji iyi vanhu vakati bvuu kuseka vanhu vese vakabva vatanga kunyomba muroora iyi itsika yagara ichiitwa kuti kana muroora achiperekwa anombonyombewa vamwe vanonyombewa kusvika vasvimha misodzi asi machuma ayiratidza kuti ane moyo wakasindimara anga akaita kuburitsa ziso rakachena kuti wekee semukaka achitarisa munhu wese ayivemo mumwe mukadzi akabva arudinhura rumbo runonzi chimhuka kwindi mwana wenyu chimhukakwindi heheheee chimhukakwindi huya uwone chimhukakwindi heheee chimhukakwindi vanhu vakasimuka kwakutanga kuimba nekutamba mamwe madzimai ayiita kutamba achiita kumuzunzira mazakwatira iye machuma wacho kana kugaya zvake ayitoratidza kuti ayinakidzwa nazvo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text"
      ],
      "metadata": {
        "id": "1aPOT_Udg6ms",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "43dbed09-a7d1-4806-a1f7-2823298cdfff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mudunhu rekwachitororo yaiva burushashike vanhu vachifarira muroora mutsva anga awuya mumusha wekwanhoto ivo vanhoto vainzwa manyuku nyuku nekuti mwanakomana wavo mukuru wayne anga avaunzira ndangaruvazhe wayne anga avakushanda mudhorobha riripedyo nedunhu ravo kubva paakapedza zvidzidzo zvake zveolevel akabva angoenda kudhorobha kunoshava akabva awana basa kwapower sales ikoko kudhorobha ndiko kwaakasangana netsvarakadenga yake yainzi rosemary chuma vakafambidzana mwedzi yakati kuti kusvika vaviri ava vapanana pamuviri vakabva vatoronga zvekutizisana machuma akabva atora vana tete vake vaviri vakava ndivo vakamuperedza pavakasvika mudunhu rachitororo vakambogara pamba pabamunini vawayne mazuva matatu muroora asati aperekwa mumusha memurume wake zuva rekuperekwa kwake rakazosvika munhu wese mudunhu iri anga akarimirira zuva iri dunhu rese rakaturunuka kuzoona svusvurandadya yanga yaunzwa nawayne mumusha wekwanhoto musikana anga akaumbwa hake iyeye ndivo vanonzi mwari vakatora nguva yavo vachinatsosika hapana murume ayisayemura kuumbwa kwarosemary anga ariruva chairo sezita rake asi wayne ndiye anga ayita rombo rakanaka kuvarume vese kuva netsvarakadenga iyi mari dzaikandwa kani munhu wese achingorumbidza mhando yemudzimai yanga yaunzwa nawayne vakaimba vakomana nevasikana muroora achifambiwa naye kubva kwabamunini baba stern kuenda kwavatezvara vake wayingoti fambe fambe obva amira mari ichikandwa pagedhi muroora ndipo paakambonetsa chose achiramba kufamba kuratidza kuti mari yaikandwa yaiva shoma vakatanga vachamutukirira kusvika vapererwa nezano vatezvara ndivo vakazobudisa chitsama chemari achibva afamba kunosvika padoor apo pakamboita nyaya kusvika vanhu vazoudza wayne kuti aburitse mari iye wayne wacho ayinzwa kuzvirova dundundu nekuti munhu wese akanga amboona rosemary ayingomurumbidza akapindwa naye muroora mumba mari ikabiswa kuti jira rakamufukidza ribiswe kumeso richibiswa kudayi munhu wese akamira hoo nerunako rwaiva namachuma madzimai akaridza mipururu nekutambambuya rukondo vakabva vaita kurova zvavo chikwee vachisimuka pavanga vakagara haiwawo wayne muzukuru wangu hapana zvawaita apa kutiunzira mapendeka emumadhorobha menyu umu guyu kutsvukira mukati mune masvosve hezvino ndiripano wakasiya ini bhogosvogodo yako uchida njuzu yemumasuweji iyi vanhu vakati bvuu kuseka vanhu vese vakabva vatanga kunyomba muroora iyi itsika yagara ichiitwa kuti kana muroora achiperekwa anombonyombewa vamwe vanonyombewa kusvika vasvimha misodzi asi machuma ayiratidza kuti ane moyo wakasindimara anga akaita kuburitsa ziso rakachena kuti wekee semukaka achitarisa munhu wese ayivemo mumwe mukadzi akabva arudinhura rumbo runonzi chimhuka kwindi mwana wenyu chimhukakwindi heheheee chimhukakwindi huya uwone chimhukakwindi heheee chimhukakwindi vanhu vakasimuka kwakutanga kuimba nekutamba mamwe madzimai ayiita kutamba achiita kumuzunzira mazakwatira iye machuma wacho kana kugaya zvake ayitoratidza kuti ayinakidzwa nazvo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vocabulary"
      ],
      "metadata": {
        "id": "VXPvbtmFhK8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using a keras tokenizer to build an optimal vocabulary\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_shona)\n",
        "max_vocab_size= 10000  # the vocabulary size\n",
        "word_index = tokenizer.word_index\n",
        "vocabulary_size = min(max_vocab_size, len(word_index))\n",
        "print(vocabulary_size)"
      ],
      "metadata": {
        "id": "l0kpZXBuhgiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4977fac1-2a9b-456b-eb20-c90984d0db00"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Embeddings"
      ],
      "metadata": {
        "id": "q7drtb8zius4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "tRquj6pMix61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c059dbad-7a5d-417a-89ab-d563b9c7b081"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "# Train word2vec model with gensim\n",
        "sentences = [sentence.split() for sentence in cleaned_text.split('.')]\n",
        "model_gensim = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "model_gensim.save(\"word2vec.model\")"
      ],
      "metadata": {
        "id": "xhdN5GafjEHC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " RNN Models with its own embedding layer"
      ],
      "metadata": {
        "id": "EZLW4AfCljKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
        "#the maximum sequence length and embedding dimension\n",
        "max_sequence_length = 100\n",
        "embedding_dim = 100\n",
        "model1 = Sequential()\n",
        "# Embedding layer\n",
        "model1.add(Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model1.add(Bidirectional(LSTM(units=150, return_sequences=True)))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(Bidirectional(LSTM(units=100)))\n",
        "model1.add(Dense(units=64, activation='relu'))\n",
        "model1.add(Dense(max_vocab_size, activation='softmax'))\n",
        "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model1.summary()\n"
      ],
      "metadata": {
        "id": "zxN5H-uAnD4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d8bb3c-b650-4f6a-d3c5-1ad53a5ff7fb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2400      \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 100, 300)          301200    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100, 300)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 200)               320800    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                12864     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10000)             650000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1287264 (4.91 MB)\n",
            "Trainable params: 1287264 (4.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Pre-Trained Embeddings"
      ],
      "metadata": {
        "id": "wfTYA7Rq6GFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#saving model 1\n",
        "model1.save('model1.h5')"
      ],
      "metadata": {
        "id": "6LM3vCp5C88c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dce9fb8-67f6-4167-ab4f-425877a6f0e3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the word2vec model\n",
        "import numpy as np\n",
        "model_gensim = Word2Vec.load(\"word2vec.model\")\n",
        "embedding_matrix = np.zeros((max_vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    try:\n",
        "        embedding_vector = model_gensim.wv[word]\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        # Word not present in gensim model, a zero embedding will be used for this word\n",
        "        pass\n",
        "\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(max_vocab_size, 100, weights=[embedding_matrix], input_length=5, trainable=False))\n",
        "model2.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(LSTM(100))\n",
        "model2.add(Dense(max_vocab_size, activation='softmax'))\n",
        "\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "8udZJPA-bFfX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
        "\n",
        "max_sequence_length = 100\n",
        "embedding_dim = 100\n",
        "\n",
        "model2 = Sequential()\n",
        "\n",
        "# Embedding layer (defined in (3))\n",
        "model2.add(Embedding(input_dim=max_vocab_size, output_dim=embedding_dim, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
        "model2.add(Bidirectional(LSTM(units=150, return_sequences=True)))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Bidirectional(LSTM(units=100)))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(units=64, activation='relu'))\n",
        "model2.add(Dense(units=max_vocab_size, activation='softmax'))\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "VY7Pkl1A-Sz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c45c70a-03a8-43af-e013-5fecc838e667"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 100, 300)          301200    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 100, 300)          0         \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirecti  (None, 200)               320800    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                12864     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10000)             650000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2284864 (8.72 MB)\n",
            "Trainable params: 1284864 (4.90 MB)\n",
            "Non-trainable params: 1000000 (3.81 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#saving model 2\n",
        "model2.save('model2.h5')"
      ],
      "metadata": {
        "id": "uUe3ebILCesn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e904140-d3c1-4dab-8d0d-409428c20f3b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "#Load the model and tokenizer\n",
        "model1=load_model('model1.h5')\n",
        "model2=load_model('model2.h5')"
      ],
      "metadata": {
        "id": "NXyVsLOzDOOn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training & Testing"
      ],
      "metadata": {
        "id": "rNXQ2vMWBn5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "input_sequences = []\n",
        "for line in cleaned_text.split('.'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=6, padding='pre'))\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = keras.utils.to_categorical(y, num_classes=max_vocab_size)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "# Training Model 1\n",
        "history1 = model1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, verbose=1)\n",
        "\n",
        "# Training Model 2\n",
        "history2 = model2.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, verbose=1)"
      ],
      "metadata": {
        "id": "7tVUy3vu_B2E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "b073e594-96f2-4a89-ffd0-f52c348c78e5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-49a266533d89>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Split the dataset into training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Training Model 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.5 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation"
      ],
      "metadata": {
        "id": "N5kfBtxvByXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss_model1 = history1.history['val_loss'][-1]\n",
        "val_loss_model2 = history2.history['val_loss'][-1]\n",
        "\n",
        "print(f\"Validation Loss for Model 1: {val_loss_model1}\")\n",
        "print(f\"Validation Loss for Model 2: {val_loss_model2}\")"
      ],
      "metadata": {
        "id": "nxo_z_4YOkX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if val_loss_model1 < val_loss_model2:\n",
        "    best_model = model1\n",
        "    best_model_name = \"best_model1.h5\"\n",
        "else:\n",
        "    best_model = model2\n",
        "    best_model_name = \"best_model2.h5\"\n",
        "\n",
        "best_model.save(best_model_name)\n",
        "print(f\"Saved the best model as {best_model_name}\")"
      ],
      "metadata": {
        "id": "1n_vpkqFPPDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_model1 = model1.count_params()\n",
        "params_model2 = model2.count_params()\n",
        "\n",
        "print(f\"Model 1 has {params_model1} parameters.\")\n",
        "print(f\"Model 2 has {params_model2} parameters.\")"
      ],
      "metadata": {
        "id": "iGoet8oMPTHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the previously saved model\n",
        "model = load_model('best_model1.h5')\n",
        "\n",
        "def predict_next_words(model, tokenizer, text, num_words=1):\n",
        "    \"\"\"\n",
        "    Predict the next set of words using the trained model.\n",
        "\n",
        "    Args:\n",
        "    - model (keras.Model): The trained model.\n",
        "    - tokenizer (Tokenizer): The tokenizer object used for preprocessing.\n",
        "    - text (str): The input text.\n",
        "    - num_words (int): The number of words to predict.\n",
        "\n",
        "    Returns:\n",
        "    - str: The predicted words.\n",
        "    \"\"\"\n",
        "    for _ in range(num_words):\n",
        "        # Tokenize and pad the text\n",
        "        sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=5, padding='pre')\n",
        "\n",
        "        # Predict the next word\n",
        "        predicted_probs = model.predict(sequence, verbose=0)\n",
        "        predicted = np.argmax(predicted_probs, axis=-1)\n",
        "\n",
        "        # Convert the predicted word index to a word\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        # Append the predicted word to the text\n",
        "        text += \" \" + output_word\n",
        "\n",
        "    return ' '.join(text.split(' ')[-num_words:])\n",
        "\n",
        "\n",
        "# Prompt the user for input\n",
        "user_input = input(\"Please type five words in Shona: \")\n",
        "\n",
        "# Predict the next words\n",
        "predicted_words = predict_next_words(model, tokenizer, user_input, num_words=3)\n",
        "print(f\"The next words might be: {predicted_words}\")"
      ],
      "metadata": {
        "id": "B6fTqYXTPWhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deploying"
      ],
      "metadata": {
        "id": "dSFp9xemPfMU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c_z4WmIbPhSE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}