{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# NLP ASSIGNMENT\n",
        "Shamiso Makainganwa\n",
        "R204437W"
      ],
      "metadata": {
        "id": "1Sy6JzJpNSfZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dUTuSHHUDMN5"
      },
      "outputs": [],
      "source": [
        "#importing and loading  all the libraries used in this assignment\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "LOADING THE SHONA TEXT"
      ],
      "metadata": {
        "id": "-sXc2PyWNQsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\" Vaitaura vachindinongedza nemudonzvo\n",
        "wavo.\n",
        "Vanhu vose vakati bvuu kuseka. Ndaida\n",
        "kupindura asi ndakanzwa vanhu vakuita\n",
        "mahon'era ndikaona kuti ndaigona kusara\n",
        "pachena. Baba vanga vagara pachituro\n",
        "chavo vakabata shaya. Mhamha\n",
        "vachipedza kuchema vakabva vashamisa\n",
        "muromo wavo ndokutanga kutaura.\n",
        "\"Pamusoroyi hama dzangu. Muchiona\n",
        "muroora wangu maChuma akagara apo,\n",
        "handina rugare pano pamusha pangu.\n",
        "Ndakatanga ndichimusiya achiita\n",
        "zvaanoda nekutaura zvaanoda. Nhasi\n",
        "chaiye ndamutuma kuchigayo\n",
        "ndikaudzwa meno amire. Ayenda\n",
        "naChido mwana wangu vavamunzira\n",
        "akakamura pagedhi rake Chido akasenga\n",
        "bhagedhi nehafu iye ndokusenga hafu.\n",
        "Pavadzoka awuya achizvivharira mumba\n",
        "make kwanzi ndaneta. Saka apa\n",
        "ndazomumutsa ndichiti awane chekudya\n",
        "nekuti anga asina kumbodya masikati\n",
        "semunhu akazvitakura. Ndazongofira\n",
        "kubvunza kuti ko asi warwara here kurara\n",
        "masikati ndipo paatanga kundituka.\n",
        "Pandangoti nditaurewo abva tanga\n",
        "kundikwapaidza mambama chaiwo.\n",
        "Mambama vanhu vaMwari hiiihiihiiii. \"\"\""
      ],
      "metadata": {
        "id": "LTEs1DGvIOc8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "bJpaAyV1ccDP",
        "outputId": "782f4402-e949-43f4-ccb8-34d480eee9b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Vaitaura vachindinongedza nemudonzvo\\nwavo.\\nVanhu vose vakati bvuu kuseka. Ndaida\\nkupindura asi ndakanzwa vanhu vakuita\\nmahon\\'era ndikaona kuti ndaigona kusara\\npachena. Baba vanga vagara pachituro\\nchavo vakabata shaya. Mhamha\\nvachipedza kuchema vakabva vashamisa\\nmuromo wavo ndokutanga kutaura.\\n\"Pamusoroyi hama dzangu. Muchiona\\nmuroora wangu maChuma akagara apo,\\nhandina rugare pano pamusha pangu.\\nNdakatanga ndichimusiya achiita\\nzvaanoda nekutaura zvaanoda. Nhasi\\nchaiye ndamutuma kuchigayo\\nndikaudzwa meno amire. Ayenda\\nnaChido mwana wangu vavamunzira\\nakakamura pagedhi rake Chido akasenga\\nbhagedhi nehafu iye ndokusenga hafu.\\nPavadzoka awuya achizvivharira mumba\\nmake kwanzi ndaneta. Saka apa\\nndazomumutsa ndichiti awane chekudya\\nnekuti anga asina kumbodya masikati\\nsemunhu akazvitakura. Ndazongofira\\nkubvunza kuti ko asi warwara here kurara\\nmasikati ndipo paatanga kundituka.\\nPandangoti nditaurewo abva tanga\\nkundikwapaidza mambama chaiwo.\\nMambama vanhu vaMwari hiiihiihiiii. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hkK0UA4fOOss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREPROCESSING TEXT"
      ],
      "metadata": {
        "id": "iXWgzEv2OPgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXpRgfR-NG5M",
        "outputId": "168f5e84-0bc0-42bd-8253-ebcc5188d76c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Join the words back into a cleaned text\n",
        "    cleaned_text = ' '.join(words)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "cleaned_text = clean_text(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIOZjSCO6Fh0",
        "outputId": "f62c188e-e578-4d5b-b00a-31dccbce9c38"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "WQK9Pr3460CD",
        "outputId": "f9abfd4f-a22e-47b7-e596-8a928229c345"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'vaitaura vachindinongedza nemudonzvo wavo vanhu vose vakati bvuu kuseka ndaida kupindura asi ndakanzwa vanhu vakuita mahonera ndikaona kuti ndaigona kusara pachena baba vanga vagara pachituro chavo vakabata shaya mhamha vachipedza kuchema vakabva vashamisa muromo wavo ndokutanga kutaura pamusoroyi hama dzangu muchiona muroora wangu machuma akagara apo handina rugare pano pamusha pangu ndakatanga ndichimusiya achiita zvaanoda nekutaura zvaanoda nhasi chaiye ndamutuma kuchigayo ndikaudzwa meno amire ayenda nachido mwana wangu vavamunzira akakamura pagedhi rake chido akasenga bhagedhi nehafu iye ndokusenga hafu pavadzoka awuya achizvivharira mumba make kwanzi ndaneta saka apa ndazomumutsa ndichiti awane chekudya nekuti anga asina kumbodya masikati semunhu akazvitakura ndazongofira kubvunza kuti ko asi warwara kurara masikati ndipo paatanga kundituka pandangoti nditaurewo abva tanga kundikwapaidza mambama chaiwo mambama vanhu vamwari hiiihiihiiii'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dfdffef8"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Tokenize and preprocess\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([cleaned_text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# tokenize the document\n",
        "result = text_to_word_sequence(text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2bXEdKZl5wM",
        "outputId": "937a3276-308a-4a66-d75b-c37d0dc36069"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['vaitaura', 'vachindinongedza', 'nemudonzvo', 'wavo', 'vanhu', 'vose', 'vakati', 'bvuu', 'kuseka', 'ndaida', 'kupindura', 'asi', 'ndakanzwa', 'vanhu', 'vakuita', \"mahon'era\", 'ndikaona', 'kuti', 'ndaigona', 'kusara', 'pachena', 'baba', 'vanga', 'vagara', 'pachituro', 'chavo', 'vakabata', 'shaya', 'mhamha', 'vachipedza', 'kuchema', 'vakabva', 'vashamisa', 'muromo', 'wavo', 'ndokutanga', 'kutaura', 'pamusoroyi', 'hama', 'dzangu', 'muchiona', 'muroora', 'wangu', 'machuma', 'akagara', 'apo', 'handina', 'rugare', 'pano', 'pamusha', 'pangu', 'ndakatanga', 'ndichimusiya', 'achiita', 'zvaanoda', 'nekutaura', 'zvaanoda', 'nhasi', 'chaiye', 'ndamutuma', 'kuchigayo', 'ndikaudzwa', 'meno', 'amire', 'ayenda', 'nachido', 'mwana', 'wangu', 'vavamunzira', 'akakamura', 'pagedhi', 'rake', 'chido', 'akasenga', 'bhagedhi', 'nehafu', 'iye', 'ndokusenga', 'hafu', 'pavadzoka', 'awuya', 'achizvivharira', 'mumba', 'make', 'kwanzi', 'ndaneta', 'saka', 'apa', 'ndazomumutsa', 'ndichiti', 'awane', 'chekudya', 'nekuti', 'anga', 'asina', 'kumbodya', 'masikati', 'semunhu', 'akazvitakura', 'ndazongofira', 'kubvunza', 'kuti', 'ko', 'asi', 'warwara', 'here', 'kurara', 'masikati', 'ndipo', 'paatanga', 'kundituka', 'pandangoti', 'nditaurewo', 'abva', 'tanga', 'kundikwapaidza', 'mambama', 'chaiwo', 'mambama', 'vanhu', 'vamwari', 'hiiihiihiiii']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# saving\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "9jLfyavKWDG3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        " BUILDING THE VOCABULARY"
      ],
      "metadata": {
        "id": "eLfMQCsFOeqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using a keras tokenizer to build an optimal vocabulary\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text)\n",
        "max_vocab_size= 10000  # the vocabulary size\n",
        "word_index = tokenizer.word_index\n",
        "vocabulary_size = min(max_vocab_size, len(word_index))\n",
        "\n",
        "\n",
        "reduced_word_index = {}\n",
        "for word, index in word_index.items():\n",
        "    if index <= vocabulary_size:\n",
        "        reduced_word_index[word] = index\n",
        "    else:\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "tokenizer.word_index = reduced_word_index\n",
        "tokenizer.word_index[tokenizer.oov_token] = vocabulary_size + 1\n",
        "tokenizer.num_words = vocabulary_size + 1\n",
        "vocabulary_size = len(word_index)\n",
        "print(\"Vocabulary size:\", vocabulary_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUQjl1jtM0vp",
        "outputId": "f4971797-b8e1-478e-a087-9ae99196b674"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WORD EMBEDDINGS"
      ],
      "metadata": {
        "id": "dwAmgyzuOvTF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "f6e79b35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebaed412-7e7e-4ec2-e693-b1a5740f81d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train word2vec model with gensim\n",
        "sentences = result\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "model.save(\"shona_embeddings.model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BI-DIRECTIONAL MODEL WITH EMBEDDING LAYER"
      ],
      "metadata": {
        "id": "gPENFQKAO6lK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8095c8fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c8d23d-2493-47d5-98cc-50202bf4e431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 5, 100)            11300     \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 5, 300)            301200    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 5, 300)            0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 113)               11413     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 484313 (1.85 MB)\n",
            "Trainable params: 484313 (1.85 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(total_words, 100, input_length=5))\n",
        "model1.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(LSTM(100))\n",
        "model1.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.save('model1.h1')"
      ],
      "metadata": {
        "id": "xAYrk6_m-Bkr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BI-DIRECTIONAL MODEL WITH PRETRAINED WORD EMBEDDINGS"
      ],
      "metadata": {
        "id": "_XH2cOyPPFOi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "90591dd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896d395a-7a42-4914-9085-540bec35c1a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 5, 100)            11300     \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 5, 300)            301200    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 5, 300)            0         \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 113)               11413     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 484313 (1.85 MB)\n",
            "Trainable params: 473013 (1.80 MB)\n",
            "Non-trainable params: 11300 (44.14 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Load the word2vec model\n",
        "import numpy as np\n",
        "model = Word2Vec.load(\"shona_embeddings.model\")\n",
        "embedding_matrix = np.zeros((total_words, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    try:\n",
        "        embedding_vector = reduced_word_index[word]\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        # Word not present in gensim model, a zero embedding will be used for this word\n",
        "        pass\n",
        "\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(total_words, 100, weights=[embedding_matrix], input_length=5, trainable=False))\n",
        "model2.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(LSTM(100))\n",
        "model2.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.save('model2.h1')"
      ],
      "metadata": {
        "id": "W47ls8Yb-rTv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = load_model('model1.h1')\n",
        "model = load_model('model2.h1')\n",
        "\n"
      ],
      "metadata": {
        "id": "il0czMAH-8w4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " MODEL TRAINING"
      ],
      "metadata": {
        "id": "Q33I0hUzPTqM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "eb129419",
        "outputId": "a5f09910-da75-40a0-dd84-9ecc49f8f295"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-0c1865f22a69>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Split the dataset into training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training model1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.4 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "input_sequences = []\n",
        "for line in cleaned_text.split('.'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=6, padding='pre'))\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = keras.utils.to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "print('training model1')\n",
        "\n",
        "# Training Model 1\n",
        "history1 = model1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=150, verbose=1)\n",
        "\n",
        "print('training model2')\n",
        "\n",
        "# Training Model 2\n",
        "history2 = model2.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=150, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL TESTING"
      ],
      "metadata": {
        "id": "UCzeaxAtPfPP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79cddef6",
        "outputId": "923cf053-480f-4a98-c1ed-7115bb13cacb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss for Model1: 11.782182693481445\n",
            "Validation Loss for Model2: 10.130215644836426\n"
          ]
        }
      ],
      "source": [
        "val_loss_model1 = history1.history['val_loss'][-1]\n",
        "val_loss_model2 = history2.history['val_loss'][-1]\n",
        "\n",
        "print(f\"Validation Loss for Model1: {val_loss_model1}\")\n",
        "print(f\"Validation Loss for Model2: {val_loss_model2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a7969ec",
        "outputId": "8a13f82a-807b-4772-8a02-79927fb36261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the best model is best_performing_model2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "if val_loss_model1 < val_loss_model2:\n",
        "    best_model = model1\n",
        "    best_model_name = \"best_performing_model1.h5\"\n",
        "else:\n",
        "    best_model = model2\n",
        "    best_model_name = \"best_performing_model2.h5\"\n",
        "\n",
        "best_model.save(best_model_name)\n",
        "print(f\"the best model is {best_model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nJfj4R2_bSxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c703656"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the previously saved model\n",
        "model = load_model('best_performing_model2.h5')\n",
        "\n",
        "def predict_next_word(model, tokenizer, text, num_words=1):\n",
        "    \"\"\"\n",
        "    Predict the next set of words using the trained model.\n",
        "\n",
        "    Args:\n",
        "    - model (keras.Model): The trained model.\n",
        "    - tokenizer (Tokenizer): The tokenizer object used for preprocessing.\n",
        "    - text (str): The input text.\n",
        "    - num_words (int): The number of words to predict.\n",
        "\n",
        "    Returns:\n",
        "    - str: The predicted words.\n",
        "    \"\"\"\n",
        "    for _ in range(num_words):\n",
        "        # Tokenize and pad the text\n",
        "        sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=5, padding='pre')\n",
        "\n",
        "        # Predict the next word\n",
        "        predicted_probs = model.predict(sequence, verbose=0)\n",
        "        predicted = np.argmax(predicted_probs, axis=-1)\n",
        "\n",
        "        # Convert the predicted word index to a word\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        # Append the predicted word to the text\n",
        "        text += \" \" + output_word\n",
        "\n",
        "    return ' '.join(text.split(' ')[-num_words:])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prompt the user for input\n",
        "\n",
        "\n",
        "# Predict the next words\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "\n",
        "  text = input(\"Nyora manzwi mashanu: \")\n",
        "\n",
        "  if text == \"0\":\n",
        "      print(\"chiitiko ichi chaperera pano .....\")\n",
        "      break\n",
        "\n",
        "  else:\n",
        "   try:\n",
        "    user_input = text.split(\" \")\n",
        "    user_input = text[-5:]\n",
        "    print(user_input)\n",
        "\n",
        "    # Prompt the user for input\n",
        "    predicted_word = predict_next_word(model, tokenizer, user_input, num_words=1)\n",
        "    # Predict the next word\n",
        "    print(f\"inzwi rawanikwa rinoteera manzwi amaisa ndeiro: {predicted_word}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   except Exception as e:\n",
        "    print(\"Error: \", e)\n",
        "    continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx7NZs1rG9gq",
        "outputId": "32f98965-a3ff-4bca-871e-4ad06d974959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nyora manzwi mashanu: manga chena inoparira parere nhema\n",
            "nhema\n",
            "inzwi rawanikwa rinoteera manzwi amaisa ndeiro: nhamo\n",
            "Nyora manzwi mashanu: 0\n",
            "chiitiko ichi chaperera pano .....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iZKjiX5BaIpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MuEaQ25eKMbS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
